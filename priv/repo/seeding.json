[
    {
        "question":  "According to the text, what is a vector?",
        "options":  [
                        "A grid of numbers, like a spreadsheet",
                        "A cluster of common information that describes a single point of data",
                        "A mathematical operation that tells how aligned two points are",
                        "The rate of change of a function at a given point"
                    ],
        "correct_answer":  "A cluster of common information that describes a single point of data",
        "motivation":  "The source material defines a vector as a \u0027cluster of common information that describes a single point of data,\u0027 using the example of a student\u0027s attributes [19, 12, 88]. A grid of numbers is a matrix, an operation for alignment is a dot product, and the rate of change is a derivative."
    },
    {
        "question":  "What does a large, positive dot product between two vectors indicate?",
        "options":  [
                        "The vectors are unrelated and geometrically perpendicular",
                        "The vectors point in opposite directions and contrast each other",
                        "The vectors point in a similar direction",
                        "The vectors represent a spreadsheet of data"
                    ],
        "correct_answer":  "The vectors point in a similar direction",
        "motivation":  "The text states, \u0027a large, positive, dot product means the vectors point in a similar direction.\u0027 A value near zero means they are unrelated (perpendicular), and a large negative value means they point in opposite directions."
    },
    {
        "question":  "What is a matrix defined as in the source material?",
        "options":  [
                        "A list of numbers describing a single student",
                        "A fundamental operation that tells us how aligned two vectors are",
                        "A grid of numbers, like a spreadsheet, often representing a collection of data points",
                        "The multi-dimensional version of a derivative"
                    ],
        "correct_answer":  "A grid of numbers, like a spreadsheet, often representing a collection of data points",
        "motivation":  "The text explains that if you stack multiple vectors (like data for 100 students) together, you get a grid of numbers, which \u0027in math, is called a matrix.\u0027 It is compared to a spreadsheet."
    },
    {
        "question":  "In the context of calculus for ML, what does a derivative, `f\u0027(x)`, tell you?",
        "options":  [
                        "The direction of the steepest ascent for a multi-dimensional function",
                        "The average value of a dataset",
                        "The rate of change or \u0027trend\u0027 of a function at any given point",
                        "How aligned two vectors are"
                    ],
        "correct_answer":  "The rate of change or \u0027trend\u0027 of a function at any given point",
        "motivation":  "The source material defines the derivative `f\u0027(x)` as telling you the function\u0027s \u0027rate of change at any given point (geometrically, that’s simply its *slope*).\u0027 This helps understand the function\u0027s \u0027trend\u0027."
    },
    {
        "question":  "What is the gradient (`∇f`) described as in the text?",
        "options":  [
                        "A function used to model errors as a \u0027hilly landscape\u0027",
                        "The multi-dimensional version of a derivative",
                        "A collection of data points in a grid",
                        "A process that starts at a random point and steps downhill"
                    ],
        "correct_answer":  "The multi-dimensional version of a derivative",
        "motivation":  "The text explicitly states that the gradient, `∇f`, \u0027is the multi-dimensional version of a derivative.\u0027 It points in the direction of the steepest ascent."
    },
    {
        "question":  "In machine learning, what is the primary goal of optimization?",
        "options":  [
                        "To find the average and spread of the data",
                        "To stack vectors into a matrix",
                        "To minimize errors and maximize results",
                        "To ensure a system is deterministic"
                    ],
        "correct_answer":  "To minimize errors and maximize results",
        "motivation":  "The text describes optimization using the \u0027min-maxing\u0027 concept: \u0027We want to minimize the errors, *and* maximize results.\u0027 This is a core problem that optimization as a field addresses."
    },
    {
        "question":  "What is a \u0027loss function\u0027 in the context of ML optimization?",
        "options":  [
                        "A function that models the \u0027hilly landscape\u0027 of errors",
                        "A function that calculates the dot product of two vectors",
                        "The algorithm used to find the minimum error",
                        "A function that represents the \u0027ground truth\u0027"
                    ],
        "correct_answer":  "A function that models the \u0027hilly landscape\u0027 of errors",
        "motivation":  "The text explains that to find the lowest valley (minimum error), \u0027we could model our errors as a function of that “hilly landscape”; we call such function a *loss function*.\u0027"
    },
    {
        "question":  "What is the core idea of the Stochastic Gradient Descent algorithm?",
        "options":  [
                        "To analytically compute the exact optimal solution",
                        "To take a small step in the direction of the gradient (uphill)",
                        "To start at a random point, calculate the gradient, and take a small step in the opposite direction (downhill)",
                        "To perfectly fit a model to every single data point in the training set"
                    ],
        "correct_answer":  "To start at a random point, calculate the gradient, and take a small step in the opposite direction (downhill)",
        "motivation":  "The text outlines the Stochastic Gradient Descent algorithm as: 1. start at a random point, 2. calculate the gradient, and 3. take a small step in the *opposite* direction (downhill). This is repeated iteratively."
    },
    {
        "question":  "Which statistical measure is described as \u0027robust to outliers\u0027?",
        "options":  [
                        "Mean",
                        "Median",
                        "Variance",
                        "Standard Deviation"
                    ],
        "correct_answer":  "Median",
        "motivation":  "The source material defines the \u0027Median\u0027 as \u0027The middle value when the data is sorted. More robust to outliers.\u0027 The Mean, in contrast, is \u0027Prone to being skewed by outliers.\u0027"
    },
    {
        "question":  "What is the key characteristic of a stochastic system?",
        "options":  [
                        "The output is perfectly predictable from the input",
                        "The same input always leads to the same output",
                        "There is inherent randomness, and the same input can lead to different outputs",
                        "It represents the \u0027ground truth\u0027 mapping"
                    ],
        "correct_answer":  "There is inherent randomness, and the same input can lead to different outputs",
        "motivation":  "The text defines stochastic systems as having \u0027inherent randomness\u0027 where the \u0027same input can lead to different outputs.\u0027 In contrast, deterministic systems are perfectly predictable."
    },
    {
        "question":  "What core feature of Elixir programming is highlighted in the text?",
        "options":  [
                        "It uses classical \u0027loops\u0027 for iteration",
                        "It has no mutability, meaning variables can only be reassigned, not changed",
                        "It is primarily used for statistical analysis",
                        "It is a deterministic systems language"
                    ],
        "correct_answer":  "It has no mutability, meaning variables can only be reassigned, not changed",
        "motivation":  "The source states, \u0027Elixir has no *mutability*, meaning: you can’t really *change* variables, only *reassign* them.\u0027 It also notes that this is why it relies on recursion instead of classical loops."
    },
    {
        "question":  "What is the \u0027paradigm shift\u0027 of Machine Learning compared to Traditional Programming?",
        "options":  [
                        "In ML, the programmer figures out all the rules explicitly",
                        "In ML, the computer is provided with data and figures out the rules on its own",
                        "In ML, the goal is to create deterministic systems",
                        "In ML, programs are written in functional languages like Elixir"
                    ],
        "correct_answer":  "In ML, the computer is provided with data and figures out the rules on its own",
        "motivation":  "The text contrasts Traditional Programming, where \u0027The programmer figures out the rules,\u0027 with Machine Learning, where \u0027we provide the computer with data... The computer then figures out the rules on its own.\u0027"
    },
    {
        "question":  "What does \u0027PAC\u0027 stand for in the context of ML models?",
        "options":  [
                        "Programming and Calculus",
                        "Pattern, Algorithm, Compute",
                        "Probably Approximately Correct",
                        "Prediction, Accuracy, Classification"
                    ],
        "correct_answer":  "Probably Approximately Correct",
        "motivation":  "The text introduces \u0027Probably Approximately Correct (PAC) models\u0027 as algorithms that \u0027work with high probability\u0027 and whose output \u0027might not be optimal,\u0027 combining \u0027approximately correct\u0027 heuristics with \u0027randomized\u0027 steps."
    },
    {
        "question":  "In the formal definition of machine learning, what is the \u0027prediction rule\u0027 or \u0027model\u0027?",
        "options":  [
                        "The set of all possible objects, `X`",
                        "The set of all possible labels, `Y`",
                        "The set of training data, `S`",
                        "A function `h: X→Y` that predicts a label for a given object"
                    ],
        "correct_answer":  "A function `h: X→Y` that predicts a label for a given object",
        "motivation":  "The text states, \u0027A learner output is a prediction rule... a function that, given an object, tries to *predict* a... label.\u0027 It is mathematically written as `h: X→Y` and is called the ML model."
    },
    {
        "question":  "What is the \u0027empirical error\u0027?",
        "options":  [
                        "The \u0027ground truth\u0027 error, which is impossible to compute",
                        "The error of the prediction rule `h` computed on the training set `S`",
                        "An error in the Elixir code",
                        "The error caused by using a linear model instead of a polynomial one"
                    ],
        "correct_answer":  "The error of the prediction rule `h` computed on the training set `S`",
        "motivation":  "The text explains that since we can\u0027t compute the \u0027perfect minimal error\u0027 (using the unknown `f`), we instead \u0027compute the error of `h` based on our training set `S`.\u0027 This is the empirical error."
    },
    {
        "question":  "In the car dealership example, what model was proposed as the *simplest* option?",
        "options":  [
                        "A high-degree polynomial model",
                        "A stochastic gradient descent model",
                        "A neural network model",
                        "A Linear Regression model"
                    ],
        "correct_answer":  "A Linear Regression model",
        "motivation":  "The text says, \u0027Let’s just propose the simplest model we can think of: Linear Regression.\u0027 The model\u0027s hypothesis is given as a straight line: `predicted_price = weight * mileage + bias`."
    },
    {
        "question":  "What is \u0027overfitting\u0027?",
        "options":  [
                        "A model that is too simple to capture the underlying trend",
                        "The process of minimizing the Mean Squared Error (MSE)",
                        "When a model \u0027memorized\u0027 the training data, including its random noise, and fails to generalize",
                        "The use of a Linear Regression model for a complex problem"
                    ],
        "correct_answer":  "When a model \u0027memorized\u0027 the training data, including its random noise, and fails to generalize",
        "motivation":  "The text describes overfitting as when the model \u0027memorized the training data, and adapted way too much, aka including its random noise, instead of “learning the general trend”.\u0027 This leads to poor performance on new data."
    },
    {
        "question":  "What is the primary characteristic of Supervised Learning?",
        "options":  [
                        "The data is unlabeled, and the machine must find hidden structures",
                        "The machine learns by interacting with an environment and receiving rewards or penalties",
                        "The data is labeled, and the goal is to learn a mapping function from inputs to outputs",
                        "The machine uses a \u0027plausibility engine\u0027 to predict the next token"
                    ],
        "correct_answer":  "The data is labeled, and the goal is to learn a mapping function from inputs to outputs",
        "motivation":  "The source defines Supervised Learning as the case \u0027When the data is labeled,\u0027 and states its goal is \u0027To learn a mapping function from inputs (X) to outputs (Y).\u0027"
    },
    {
        "question":  "What is the core task of \u0027Unsupervised Learning\u0027?",
        "options":  [
                        "To predict a continuous value, like a house price",
                        "To learn from labeled \u0027ground truth\u0027 data",
                        "To find hidden structures or patterns in unlabeled data",
                        "To learn a policy through trial and error with rewards"
                    ],
        "correct_answer":  "To find hidden structures or patterns in unlabeled data",
        "motivation":  "The text states that in Unsupervised Learning, \u0027The data is unlabeled\u0027 and the \u0027Goal: To find hidden structures and patterns in data.\u0027 This is contrasted with Supervised Learning, which uses labeled data."
    },
    {
        "question":  "How is \u0027Reinforcement Learning\u0027 (RL) described?",
        "options":  [
                        "As a method for finding patterns in unlabeled data",
                        "As a method for learning a mapping from inputs (X) to outputs (Y)",
                        "As an \u0027agent\u0027 learning through trial and error by interacting with an environment to maximize a \u0027reward\u0027",
                        "As a system that is perfectly predictable and involves no randomness"
                    ],
        "correct_answer":  "As an \u0027agent\u0027 learning through trial and error by interacting with an environment to maximize a \u0027reward\u0027",
        "motivation":  "The text describes Reinforcement Learning as an \u0027Agent\u0027 that \u0027interacts with an Environment.\u0027 It \u0027learns by trial and error\u0027 based on \u0027Rewards\u0027 or \u0027Penalties\u0027 with the \u0027Goal: To learn the best “policy”... that maximizes its cumulative reward over time.\u0027"
    },
    {
        "question":  "What is the \u0027garbage in, garbage out\u0027 (GIGO) principle in ML?",
        "options":  [
                        "The idea that ML models can fix bad data through optimization",
                        "The fact that the quality of an ML model\u0027s output is entirely dependent on the quality of its input data",
                        "A type of model used for sorting waste",
                        "The process of cleaning and preprocessing data"
                    ],
        "correct_answer":  "The fact that the quality of an ML model\u0027s output is entirely dependent on the quality of its input data",
        "motivation":  "The text introduces \u0027Garbage in, Garbage out\u0027 to mean \u0027the quality of your data *directly* determines the quality of your model.\u0027 If the input data is flawed, the model\u0027s predictions will be, too."
    },
    {
        "question":  "What is an example of \u0027Biased Data\u0027 given in the text?",
        "options":  [
                        "A sensor recording an impossible temperature",
                        "A customer dataset where half the age entries are blank",
                        "A hiring system trained on data where 90% of past hires were male, leading the model to favor male candidates",
                        "Including a student\u0027s \u0027shoe size\u0027 to predict their exam score"
                    ],
        "correct_answer":  "A hiring system trained on data where 90% of past hires were male, leading the model to favor male candidates",
        "motivation":  "The text provides this exact example for Biased Data: \u0027a hiring ML system uses data... where, historically, 90% of engineers hired were male: the model will likely learn that being male is a key characteristic... and will be biased against qualified female candidates.\u0027"
    },
    {
        "question":  "In the core ML workflow, what is \u0027Preprocessing\u0027?",
        "options":  [
                        "The final step where the model is put into a production environment",
                        "The step where the model \u0027learns\u0027 the patterns from data",
                        "Gathering the raw data from databases, APIs, or files",
                        "Cleaning, formatting, and transforming data into a usable state"
                    ],
        "correct_answer":  "Cleaning, formatting, and transforming data into a usable state",
        "motivation":  "The source material\u0027s \u0027core workflow\u0027 lists Preprocessing as the step that \u0027involves cleaning the data (handling missing values, removing duplicates), formatting it, and transforming it into a usable state.\u0027"
    },
    {
        "question":  "What is the purpose of the \u0027validation set\u0027 in a training framework?",
        "options":  [
                        "It is the main dataset used to \u0027learn\u0027 the model parameters (like weights)",
                        "It is used for the final, one-time evaluation of the *best* model\u0027s performance",
                        "It is used to compare different models or tune hyperparameters, to see which model generalizes best",
                        "It is a dataset of \u0027ground truth\u0027 that is never used"
                    ],
        "correct_answer":  "It is used to compare different models or tune hyperparameters, to see which model generalizes best",
        "motivation":  "The text explains the validation set is used \u0027to *validate* which one [model] we should pick\u0027 and to tune \u0027hyperparameters.\u0027 It\u0027s for model selection, unlike the training set (for learning) or the test set (for final evaluation)."
    },
    {
        "question":  "What is the \u0027test set\u0027 used for?",
        "options":  [
                        "To train the model on the largest portion of data",
                        "To compare different models against each other to pick the best one",
                        "To provide a final, unbiased assessment of the *chosen* model\u0027s performance on unseen data",
                        "To find hidden patterns in unlabeled data"
                    ],
        "correct_answer":  "To provide a final, unbiased assessment of the *chosen* model\u0027s performance on unseen data",
        "motivation":  "The text states the test set is \u0027a final, held-back, *untouched* dataset.\u0027 It \u0027is used only *once*\u0027 to \u0027get a final, unbiased measure of how our *chosen* model... will perform in the real world.\u0027"
    },
    {
        "question":  "According to the text, what is a vector?",
        "options":  [
                        "A grid of numbers, like a spreadsheet",
                        "A cluster of common information that describes a single point of data",
                        "A mathematical operation that tells how aligned two points are",
                        "The rate of change of a function at a given point"
                    ],
        "correct_answer":  "A cluster of common information that describes a single point of data",
        "motivation":  "The source material defines a vector as a \u0027cluster of common information that describes a single point of data,\u0027 using the example of a student\u0027s attributes [19, 12, 88]. A grid of numbers is a matrix, an operation for alignment is a dot product, and the rate of change is a derivative."
    },
    {
        "question":  "What does a large, positive dot product between two vectors indicate?",
        "options":  [
                        "The vectors are unrelated and geometrically perpendicular",
                        "The vectors point in opposite directions and contrast each other",
                        "The vectors point in a similar direction",
                        "The vectors represent a spreadsheet of data"
                    ],
        "correct_answer":  "The vectors point in a similar direction",
        "motivation":  "The text states, \u0027a large, positive, dot product means the vectors point in a similar direction.\u0027 A value near zero means they are unrelated (perpendicular), and a large negative value means they point in opposite directions."
    },
    {
        "question":  "What is a matrix defined as in the source material?",
        "options":  [
                        "A list of numbers describing a single student",
                        "A fundamental operation that tells us how aligned two vectors are",
                        "A grid of numbers, like a spreadsheet, often representing a collection of data points",
                        "The multi-dimensional version of a derivative"
                    ],
        "correct_answer":  "A grid of numbers, like a spreadsheet, often representing a collection of data points",
        "motivation":  "The text explains that if you stack multiple vectors (like data for 100 students) together, you get a grid of numbers, which \u0027in math, is called a matrix.\u0027 It is compared to a spreadsheet."
    },
    {
        "question":  "In the context of calculus for ML, what does a derivative, `f\u0027(x)`, tell you?",
        "options":  [
                        "The direction of the steepest ascent for a multi-dimensional function",
                        "The average value of a dataset",
                        "The rate of change or \u0027trend\u0027 of a function at any given point",
                        "How aligned two vectors are"
                    ],
        "correct_answer":  "The rate of change or \u0027trend\u0027 of a function at any given point",
        "motivation":  "The source material defines the derivative `f\u0027(x)` as telling you the function\u0027s \u0027rate of change at any given point (geometrically, that’s simply its *slope*).\u0027 This helps understand the function\u0027s \u0027trend\u0027."
    },
    {
        "question":  "What is the gradient (`∇f`) described as in the text?",
        "options":  [
                        "A function used to model errors as a \u0027hilly landscape\u0027",
                        "The multi-dimensional version of a derivative",
                        "A collection of data points in a grid",
                        "A process that starts at a random point and steps downhill"
                    ],
        "correct_answer":  "The multi-dimensional version of a derivative",
        "motivation":  "The text explicitly states that the gradient, `∇f`, \u0027is the multi-dimensional version of a derivative.\u0027 It points in the direction of the steepest ascent."
    },
    {
        "question":  "In machine learning, what is the primary goal of optimization?",
        "options":  [
                        "To find the average and spread of the data",
                        "To stack vectors into a matrix",
                        "To minimize errors and maximize results",
                        "To ensure a system is deterministic"
                    ],
        "correct_answer":  "To minimize errors and maximize results",
        "motivation":  "The text describes optimization using the \u0027min-maxing\u0027 concept: \u0027We want to minimize the errors, *and* maximize results.\u0027 This is a core problem that optimization as a field addresses."
    },
    {
        "question":  "What is a \u0027loss function\u0027 in the context of ML optimization?",
        "options":  [
                        "A function that models the \u0027hilly landscape\u0027 of errors",
                        "A function that calculates the dot product of two vectors",
                        "The algorithm used to find the minimum error",
                        "A function that represents the \u0027ground truth\u0027"
                    ],
        "correct_answer":  "A function that models the \u0027hilly landscape\u0027 of errors",
        "motivation":  "The text explains that to find the lowest valley (minimum error), \u0027we could model our errors as a function of that “hilly landscape”; we call such function a *loss function*.\u0027"
    },
    {
        "question":  "What is the core idea of the Stochastic Gradient Descent algorithm?",
        "options":  [
                        "To analytically compute the exact optimal solution",
                        "To take a small step in the direction of the gradient (uphill)",
                        "To start at a random point, calculate the gradient, and take a small step in the opposite direction (downhill)",
                        "To perfectly fit a model to every single data point in the training set"
                    ],
        "correct_answer":  "To start at a random point, calculate the gradient, and take a small step in the opposite direction (downhill)",
        "motivation":  "The text outlines the Stochastic Gradient Descent algorithm as: 1. start at a random point, 2. calculate the gradient, and 3. take a small step in the *opposite* direction (downhill). This is repeated iteratively."
    },
    {
        "question":  "Which statistical measure is described as \u0027robust to outliers\u0027?",
        "options":  [
                        "Mean",
                        "Median",
                        "Variance",
                        "Standard Deviation"
                    ],
        "correct_answer":  "Median",
        "motivation":  "The source material defines the \u0027Median\u0027 as \u0027The middle value when the data is sorted. More robust to outliers.\u0027 The Mean, in contrast, is \u0027Prone to being skewed by outliers.\u0027"
    },
    {
        "question":  "What is the key characteristic of a stochastic system?",
        "options":  [
                        "The output is perfectly predictable from the input",
                        "The same input always leads to the same output",
                        "There is inherent randomness, and the same input can lead to different outputs",
                        "It represents the \u0027ground truth\u0027 mapping"
                    ],
        "correct_answer":  "There is inherent randomness, and the same input can lead to different outputs",
        "motivation":  "The text defines stochastic systems as having \u0027inherent randomness\u0027 where the \u0027same input can lead to different outputs.\u0027 In contrast, deterministic systems are perfectly predictable."
    },
    {
        "question":  "What core feature of Elixir programming is highlighted in the text?",
        "options":  [
                        "It uses classical \u0027loops\u0027 for iteration",
                        "It has no mutability, meaning variables can only be reassigned, not changed",
                        "It is primarily used for statistical analysis",
                        "It is a deterministic systems language"
                    ],
        "correct_answer":  "It has no mutability, meaning variables can only be reassigned, not changed",
        "motivation":  "The source states, \u0027Elixir has no *mutability*, meaning: you can’t really *change* variables, only *reassign* them.\u0027 It also notes that this is why it relies on recursion instead of classical loops."
    },
    {
        "question":  "What is the \u0027paradigm shift\u0027 of Machine Learning compared to Traditional Programming?",
        "options":  [
                        "In ML, the programmer figures out all the rules explicitly",
                        "In ML, the computer is provided with data and figures out the rules on its own",
                        "In ML, the goal is to create deterministic systems",
                        "In ML, programs are written in functional languages like Elixir"
                    ],
        "correct_answer":  "In ML, the computer is provided with data and figures out the rules on its own",
        "motivation":  "The text contrasts Traditional Programming, where \u0027The programmer figures out the rules,\u0027 with Machine Learning, where \u0027we provide the computer with data... The computer then figures out the rules on its own.\u0027"
    },
    {
        "question":  "What does \u0027PAC\u0027 stand for in the context of ML models?",
        "options":  [
                        "Programming and Calculus",
                        "Pattern, Algorithm, Compute",
                        "Probably Approximately Correct",
                        "Prediction, Accuracy, Classification"
                    ],
        "correct_answer":  "Probably Approximately Correct",
        "motivation":  "The text introduces \u0027Probably Approximately Correct (PAC) models\u0027 as algorithms that \u0027work with high probability\u0027 and whose output \u0027might not be optimal,\u0027 combining \u0027approximately correct\u0027 heuristics with \u0027randomized\u0027 steps."
    },
    {
        "question":  "In the formal definition of machine learning, what is the \u0027prediction rule\u0027 or \u0027model\u0027?",
        "options":  [
                        "The set of all possible objects, `X`",
                        "The set of all possible labels, `Y`",
                        "The set of training data, `S`",
                        "A function `h: X→Y` that predicts a label for a given object"
                    ],
        "correct_answer":  "A function `h: X→Y` that predicts a label for a given object",
        "motivation":  "The text states, \u0027A learner output is a prediction rule... a function that, given an object, tries to *predict* a... label.\u0027 It is mathematically written as `h: X→Y` and is called the ML model."
    },
    {
        "question":  "What is the \u0027empirical error\u0027?",
        "options":  [
                        "The \u0027ground truth\u0027 error, which is impossible to compute",
                        "The error of the prediction rule `h` computed on the training set `S`",
                        "An error in the Elixir code",
                        "The error caused by using a linear model instead of a polynomial one"
                    ],
        "correct_answer":  "The error of the prediction rule `h` computed on the training set `S`",
        "motivation":  "The text explains that since we can\u0027t compute the \u0027perfect minimal error\u0027 (using the unknown `f`), we instead \u0027compute the error of `h` based on our training set `S`.\u0027 This is the empirical error."
    },
    {
        "question":  "In the car dealership example, what model was proposed as the *simplest* option?",
        "options":  [
                        "A high-degree polynomial model",
                        "A stochastic gradient descent model",
                        "A neural network model",
                        "A Linear Regression model"
                    ],
        "correct_answer":  "A Linear Regression model",
        "motivation":  "The text says, \u0027Let’s just propose the simplest model we can think of: Linear Regression.\u0027 The model\u0027s hypothesis is given as a straight line: `predicted_price = weight * mileage + bias`."
    },
    {
        "question":  "What is \u0027overfitting\u0027?",
        "options":  [
                        "A model that is too simple to capture the underlying trend",
                        "The process of minimizing the Mean Squared Error (MSE)",
                        "When a model \u0027memorized\u0027 the training data, including its random noise, and fails to generalize",
                        "The use of a Linear Regression model for a complex problem"
                    ],
        "correct_answer":  "When a model \u0027memorized\u0027 the training data, including its random noise, and fails to generalize",
        "motivation":  "The text describes overfitting as when the model \u0027memorized the training data, and adapted way too much, aka including its random noise, instead of “learning the general trend”.\u0027 This leads to poor performance on new data."
    },
    {
        "question":  "What is the primary characteristic of Supervised Learning?",
        "options":  [
                        "The data is unlabeled, and the machine must find hidden structures",
                        "The machine learns by interacting with an environment and receiving rewards or penalties",
                        "The data is labeled, and the goal is to learn a mapping function from inputs to outputs",
                        "The machine uses a \u0027plausibility engine\u0027 to predict the next token"
                    ],
        "correct_answer":  "The data is labeled, and the goal is to learn a mapping function from inputs to outputs",
        "motivation":  "The source defines Supervised Learning as the case \u0027When the data is labeled,\u0027 and states its goal is \u0027To learn a mapping function from inputs (X) to outputs (Y).\u0027"
    },
    {
        "question":  "What is the core task of \u0027Unsupervised Learning\u0027?",
        "options":  [
                        "To predict a continuous value, like a house price",
                        "To learn from labeled \u0027ground truth\u0027 data",
                        "To find hidden structures or patterns in unlabeled data",
                        "To learn a policy through trial and error with rewards"
                    ],
        "correct_answer":  "To find hidden structures or patterns in unlabeled data",
        "motivation":  "The text states that in Unsupervised Learning, \u0027The data is unlabeled\u0027 and the \u0027Goal: To find hidden structures and patterns in data.\u0027 This is contrasted with Supervised Learning, which uses labeled data."
    },
    {
        "question":  "How is \u0027Reinforcement Learning\u0027 (RL) described?",
        "options":  [
                        "As a method for finding patterns in unlabeled data",
                        "As a method for learning a mapping from inputs (X) to outputs (Y)",
                        "As an \u0027agent\u0027 learning through trial and error by interacting with an environment to maximize a \u0027reward\u0027",
                        "As a system that is perfectly predictable and involves no randomness"
                    ],
        "correct_answer":  "As an \u0027agent\u0027 learning through trial and error by interacting with an environment to maximize a \u0027reward\u0027",
        "motivation":  "The text describes Reinforcement Learning as an \u0027Agent\u0027 that \u0027interacts with an Environment.\u0027 It \u0027learns by trial and error\u0027 based on \u0027Rewards\u0027 or \u0027Penalties\u0027 with the \u0027Goal: To learn the best “policy”... that maximizes its cumulative reward over time.\u0027"
    },
    {
        "question":  "What is the \u0027garbage in, garbage out\u0027 (GIGO) principle in ML?",
        "options":  [
                        "The idea that ML models can fix bad data through optimization",
                        "The fact that the quality of an ML model\u0027s output is entirely dependent on the quality of its input data",
                        "A type of model used for sorting waste",
                        "The process of cleaning and preprocessing data"
                    ],
        "correct_answer":  "The fact that the quality of an ML model\u0027s output is entirely dependent on the quality of its input data",
        "motivation":  "The text introduces \u0027Garbage in, Garbage out\u0027 to mean \u0027the quality of your data *directly* determines the quality of your model.\u0027 If the input data is flawed, the model\u0027s predictions will be, too."
    },
    {
        "question":  "What is an example of \u0027Biased Data\u0027 given in the text?",
        "options":  [
                        "A sensor recording an impossible temperature",
                        "A customer dataset where half the age entries are blank",
                        "A hiring system trained on data where 90% of past hires were male, leading the model to favor male candidates",
                        "Including a student\u0027s \u0027shoe size\u0027 to predict their exam score"
                    ],
        "correct_answer":  "A hiring system trained on data where 90% of past hires were male, leading the model to favor male candidates",
        "motivation":  "The text provides this exact example for Biased Data: \u0027a hiring ML system uses data... where, historically, 90% of engineers hired were male: the model will likely learn that being male is a key characteristic... and will be biased against qualified female candidates.\u0027"
    },
    {
        "question":  "In the core ML workflow, what is \u0027Preprocessing\u0027?",
        "options":  [
                        "The final step where the model is put into a production environment",
                        "The step where the model \u0027learns\u0027 the patterns from data",
                        "Gathering the raw data from databases, APIs, or files",
                        "Cleaning, formatting, and transforming data into a usable state"
                    ],
        "correct_answer":  "Cleaning, formatting, and transforming data into a usable state",
        "motivation":  "The source material\u0027s \u0027core workflow\u0027 lists Preprocessing as the step that \u0027involves cleaning the data (handling missing values, removing duplicates), formatting it, and transforming it into a usable state.\u0027"
    },
    {
        "question":  "What is the purpose of the \u0027validation set\u0027 in a training framework?",
        "options":  [
                        "It is the main dataset used to \u0027learn\u0027 the model parameters (like weights)",
                        "It is used for the final, one-time evaluation of the *best* model\u0027s performance",
                        "It is used to compare different models or tune hyperparameters, to see which model generalizes best",
                        "It is a dataset of \u0027ground truth\u0027 that is never used"
                    ],
        "correct_answer":  "It is used to compare different models or tune hyperparameters, to see which model generalizes best",
        "motivation":  "The text explains the validation set is used \u0027to *validate* which one [model] we should pick\u0027 and to tune \u0027hyperparameters.\u0027 It\u0027s for model selection, unlike the training set (for learning) or the test set (for final evaluation)."
    },
    {
        "question":  "What is the \u0027test set\u0027 used for?",
        "options":  [
                        "To train the model on the largest portion of data",
                        "To compare different models against each other to pick the best one",
                        "To provide a final, unbiased assessment of the *chosen* model\u0027s performance on unseen data",
                        "To find hidden patterns in unlabeled data"
                    ],
        "correct_answer":  "To provide a final, unbiased assessment of the *chosen* model\u0027s performance on unseen data",
        "motivation":  "The text states the test set is \u0027a final, held-back, *untouched* dataset.\u0027 It \u0027is used only *once*\u0027 to \u0027get a final, unbiased measure of how our *chosen* model... will perform in the real world.\u0027"
    },
    {
        "question":  "In the car dealership example, what is the \u0027weight\u0027 in the linear model `predicted_price = weight * mileage + bias`?",
        "options":  [
                        "The starting price of all cars",
                        "A parameter that represents how much the price changes for each additional km of mileage",
                        "The average mileage of all cars in the dataset",
                        "The final predicted price of the car"
                    ],
        "correct_answer":  "A parameter that represents how much the price changes for each additional km of mileage",
        "motivation":  "In the linear regression model `predicted_price = weight * mileage + bias`, the \u0027weight\u0027 (or slope) determines the relationship between mileage and price. A negative weight, as in the example `-0.14`, means the price decreases by that amount for each unit increase in mileage."
    },
    {
        "question":  "What does the text identify as the main goal of a machine learning model, which differs from having a low training error?",
        "options":  [
                        "To achieve a training error of exactly zero",
                        "To create the most complex model possible, like a high-degree polynomial",
                        "To find a model `h` that performs well in real-world scenarios on new, unseen data",
                        "To perfectly memorize all the data points in the training set"
                    ],
        "correct_answer":  "To find a model `h` that performs well in real-world scenarios on new, unseen data",
        "motivation":  "The text explicitly states that \u0027having a low training error isn\u0027t our primary goal.\u0027 The overfitting example demonstrates that a zero-error model can be useless. The true goal is to \u0027find a model `h` so that it can perform nicely in real world scenarios\u0027 (generalization)."
    },
    {
        "question":  "In the formal definition of Reinforcement Learning, what is a \u0027policy\u0027?",
        "options":  [
                        "The reward or penalty the agent receives",
                        "The agent\u0027s strategy for choosing an action based on the current state",
                        "The set of all possible states in the environment",
                        "The environment the agent interacts with"
                    ],
        "correct_answer":  "The agent\u0027s strategy for choosing an action based on the current state",
        "motivation":  "The text defines the \u0027Policy (π)\u0027 in Reinforcement Learning as the \u0027Agent’s strategy. This is a function that maps a state (S) to an action (A).\u0027 The agent\u0027s goal is to learn the *best* policy."
    },
    {
        "question":  "What is an example of \u0027Missing Data\u0027 provided in the text?",
        "options":  [
                        "A sensor recording -200°C in July",
                        "A hiring dataset that is 90% male",
                        "Customer information where half the entries for age are blank",
                        "A house listed with 10 square meters and 15 bedrooms"
                    ],
        "correct_answer":  "Customer information where half the entries for age are blank",
        "motivation":  "The text gives a specific example of Missing Data: \u0027customer information where half the entries for age are blank, while being interested in its purchasing habits.\u0027 The sensor and house examples relate to \u0027Incorrect or Inaccurate Data,\u0027 and the hiring example relates to \u0027Biased Data.\u0027"
    },
    {
        "question":  "What is the purpose of \u0027K-Fold Cross-Validation\u0027?",
        "options":  [
                        "To deploy the model to production",
                        "To gather the initial raw data",
                        "To get a more robust estimate of model performance and avoid bias from a single train/validation split",
                        "To clean and format the data before training"
                    ],
        "correct_answer":  "To get a more robust estimate of model performance and avoid bias from a single train/validation split",
        "motivation":  "The text explains that K-Fold Cross-Validation is used \u0027to get a more robust estimate\u0027 of a model\u0027s performance. It involves splitting the data into \u0027K\u0027 folds and training/evaluating the model \u0027K\u0027 times, each time using a different fold as the validation set, then averaging the results."
    },
    {
        "question":  "What 2017 Google paper introduced the \u0027Transformer\u0027 architecture, changing the field of LLMs?",
        "options":  [
                        "\"The Unreasonable Effectiveness of Recurrent Neural Networks\"",
                        "\"Attention Is All You Need\"",
                        "\"Deep Learning for Natural Language Processing\"",
                        "\"Reinforcement Learning from Human Feedback\""
                    ],
        "correct_answer":  "\"Attention Is All You Need\"",
        "motivation":  "The text identifies Phase 4 of LLM history as starting in 2017 with the Google paper titled \u0027\"Attention Is All You Need\"\u0027. This paper \u0027introduced a new architecture: the Transformer.\u0027"
    },
    {
        "question":  "What is the core idea of \u0027self-attention\u0027 in the Transformer architecture?",
        "options":  [
                        "To process text one word at a time, keeping a \u0027singleton memory\u0027",
                        "To break text down into numerical tokens",
                        "To figure out which words in a sentence are most important to which other words, processing the context all at once",
                        "To fine-tune the model on a specific, supervised task"
                    ],
        "correct_answer":  "To figure out which words in a sentence are most important to which other words, processing the context all at once",
        "motivation":  "The text explains that the Transformer\u0027s core idea was to \u0027process and find the entire sentence context at once.\u0027 The mechanism for this is \u0027self-attention,\u0027 and its \u0027job is to figure out which words are most important to which other words.\u0027"
    },
    {
        "question":  "In an LLM architecture, what is \u0027Tokenization\u0027?",
        "options":  [
                        "Looking up a token in a giant dictionary to get a \u0027Meaning Vector\u0027",
                        "Training the model to predict the next token in a sequence",
                        "The process of breaking text down into numbers (tokens) that the model can read",
                        "Using human feedback to rank the model\u0027s outputs"
                    ],
        "correct_answer":  "The process of breaking text down into numbers (tokens) that the model can read",
        "motivation":  "The text describes \u0027Tokenization\u0027 as the first step in the LLM architecture: \u0027the model can\u0027t read “words”... We need to break text down into numbers. These are called “tokens”.\u0027"
    },
    {
        "question":  "What is an \u0027Embedding\u0027 in the context of LLMs?",
        "options":  [
                        "The final output of the model after fine-tuning",
                        "The process of breaking text into tokens",
                        "A vector that represents a token\u0027s \u0027position\u0027 in a multi-dimensional \u0027concept space\u0027",
                        "A security vulnerability where a user tricks the model"
                    ],
        "correct_answer":  "A vector that represents a token\u0027s \u0027position\u0027 in a multi-dimensional \u0027concept space\u0027",
        "motivation":  "The text defines an \u0027embedding\u0027 as the step after tokenization. The model looks up each token in a table that \u0027simply contains *a vector* for each token. This vector represents the token\u0027s \"position\" in a multi-dimensional “concept space”.\u0027"
    },
    {
        "question":  "What is \u0027Pre-training\u0027 in the LLM architecture?",
        "options":  [
                        "The final step where human feedback is used to refine the model\u0027s helpfulness",
                        "The main *unsupervised* learning phase where the model learns by predicting the next token in a massive dataset",
                        "The *supervised* learning phase where the model is trained on specific tasks like translation or summarization",
                        "The process of creating \u0027Meaning Vectors\u0027 (embeddings) for tokens"
                    ],
        "correct_answer":  "The main *unsupervised* learning phase where the model learns by predicting the next token in a massive dataset",
        "motivation":  "The text describes \u0027Pre-training\u0027 as \u0027the biggest, unsupervised, training step\u0027 where the model \u0027is fed *all* the data... Its only goal is to predict the next token.\u0027 This is contrasted with \u0027Fine-Tuning,\u0027 which is supervised."
    },
    {
        "question":  "What is \u0027RLHF\u0027 (Reinforcement Learning from Human Feedback)?",
        "options":  [
                        "The initial unsupervised training step on web-scale data",
                        "The process of breaking text into numerical tokens",
                        "A 2017 paper that introduced the Transformer architecture",
                        "A fine-tuning step where human rankings of model outputs are used to train a \u0027reward model\u0027 to make the LLM more helpful and aligned"
                    ],
        "correct_answer":  "A fine-tuning step where human rankings of model outputs are used to train a \u0027reward model\u0027 to make the LLM more helpful and aligned",
        "motivation":  "The text explains \u0027RLHF\u0027 as a step after \u0027Fine-Tuning.\u0027 It involves \u0027humans... ranking the outputs.\u0027 This data is used to \u0027train a *new* “reward model”\u0027 which is then used with Reinforcement Learning \u0027to make the LLM “better” (more helpful, less toxic, etc.).\u0027"
    },
    {
        "question":  "How does the text define LLM \u0027Hallucinations\u0027?",
        "options":  [
                        "A deliberate attempt by the LLM to deceive the user",
                        "The LLM admitting that it does not know an answer",
                        "When the LLM confidently produces factually incorrect information or makes up sources",
                        "A type of security attack to trick the model"
                    ],
        "correct_answer":  "When the LLM confidently produces factually incorrect information or makes up sources",
        "motivation":  "The text defines \u0027Hallucinations\u0027 as when an LLM \u0027will confidently... make up facts\u0027 or cite \u0027a non-existent book, paper or documentation bit.\u0027 It emphasizes that this is not \u0027lying\u0027 because the LLM has no intent, it\u0027s just a \u0027plausibility engine\u0027 generating a statistically likely (but factually incorrect) sequence."
    },
    {
        "question":  "Why does the text call an LLM a \u0027plausibility engine,\u0027 not a \u0027truth engine\u0027?",
        "options":  [
                        "Because its primary goal is to retrieve factual information from a database",
                        "Because it is trained to predict the next plausible token in a sequence, not to verify the factual truth of its statements",
                        "Because it has a \u0027knowledge cutoff\u0027 and cannot access new information",
                        "Because it is designed to be a creative writing partner"
                    ],
        "correct_answer":  "Because it is trained to predict the next plausible token in a sequence, not to verify the factual truth of its statements",
        "motivation":  "The text explicitly states: \u0027Recall that its pre-trained model goal is to predict the next plausible token in a sequence. **That’s it**. It is a “plausibility engine”, not a “truth engine”.\u0027 This explains *why* hallucinations happen."
    },
    {
        "question":  "What is a \u0027Prompt Injection\u0027 attack?",
        "options":  [
                        "When an LLM hallucinates and provides incorrect code",
                        "When an LLM\u0027s knowledge is outdated due to its training cutoff",
                        "When a malicious user crafts input to trick the model into ignoring the developer\u0027s instructions and following new, hidden ones",
                        "The process of fine-tuning an LLM for a specific task"
                    ],
        "correct_answer":  "When a malicious user crafts input to trick the model into ignoring the developer\u0027s instructions and following new, hidden ones",
        "motivation":  "The text defines \u0027Prompt injection\u0027 as \u0027an attack where a malicious user crafts their input to trick the model into ignoring the developer\u0027s original instructions and following the user\u0027s new, hidden instructions instead.\u0027"
    },
    {
        "question":  "According to the text, why are LLMs vulnerable to prompt injections?",
        "options":  [
                        "Because they have a knowledge cutoff",
                        "Because they are \u0027plausibility engines\u0027",
                        "Because the LLM cannot fundamentally distinguish between instructions (from a developer) and data (from the user)",
                        "Because they are not trained with RLHF"
                    ],
        "correct_answer":  "Because the LLM cannot fundamentally distinguish between instructions (from a developer) and data (from the user)",
        "motivation":  "The text explains this vulnerability: \u0027...the LLM cannot fundamentally distinguish between instructions (from a developer) and data (from the user). In the end, it\u0027s all just tokens.\u0027 This is the core reason the attack works."
    },
    {
        "question":  "What is the \u0027knowledge cutoff\u0027 of an LLM?",
        "options":  [
                        "The LLM\u0027s inability to understand complex topics",
                        "A security measure to prevent prompt injections",
                        "The fact that an LLM\u0027s knowledge is static and based on its pre-training data, which is from a specific point in the past",
                        "The LLM\u0027s tendency to hallucinate facts"
                    ],
        "correct_answer":  "The fact that an LLM\u0027s knowledge is static and based on its pre-training data, which is from a specific point in the past",
        "motivation":  "The text defines the \u0027knowledge cutoff\u0027 as the state that \u0027its knowledge is not live.\u0027 It is a \u0027static snapshot of the pre-training data\u0027 from \u0027some point in the past,\u0027 and \u0027the model won’t learn anything since.\u0027"
    },
    {
        "question":  "What is \u0027automation bias\u0027 in the context of over-relying on LLMs?",
        "options":  [
                        "The LLM\u0027s bias towards certain topics based on its training data",
                        "The assumption that because the model is articulate and confident, it must be right",
                        "A type of prompt injection attack",
                        "The process of using an LLM to automate software development"
                    ],
        "correct_answer":  "The assumption that because the model is articulate and confident, it must be right",
        "motivation":  "The text describes \u0027automation bias\u0027 as a consequence of over-reliance: \u0027Because the model is so articulate and confident, we develop an “automation bias”: we just assume it’s right.\u0027 This is part of the \u0027ChatGPT told me so\u0027 problem."
    },
    {
        "question":  "What is the text\u0027s correction to the myth \u0027An LLM is just a fancier autocomplete\u0027?",
        "options":  [
                        "This is correct; it is just a larger autocomplete.",
                        "An LLM\u0027s scale unlocks \u0027emergent abilities\u0027—complex behaviors not explicitly programmed, like step-by-step reasoning or writing code.",
                        "An LLM is a \u0027truth engine,\u0027 not an autocomplete tool.",
                        "An LLM is an information retrieval system, which is different from autocomplete."
                    ],
        "correct_answer":  "An LLM\u0027s scale unlocks \u0027emergent abilities\u0027—complex behaviors not explicitly programmed, like step-by-step reasoning or writing code.",
        "motivation":  "The text refutes this myth by stating it\u0027s an \u0027over-simplification.\u0027 It explains that \u0027emergent abilities\u0027 like \u0027step-by-step reasoning (chain-of-thought), writing functional code, and even passing standardized exams\u0027 appear \u0027once the models become large enough.\u0027 This makes them \u0027far more powerful and flexible than simple autocomplete.\u0027"
    },
    {
        "question":  "How does the text differentiate a Search Engine from an LLM?",
        "options":  [
                        "They are the same; an LLM is just a better search engine.",
                        "A Search Engine is a \u0027generator tool\u0027 and an LLM is a \u0027search tool\u0027.",
                        "A Search Engine is an \u0027Information Retrieval system\u0027 (finds *existing* info), while an LLM is an \u0027Information Generation system\u0027 (produces *new* tokens).",
                        "A Search Engine is vulnerable to hallucinations, while an LLM is not."
                    ],
        "correct_answer":  "A Search Engine is an \u0027Information Retrieval system\u0027 (finds *existing* info), while an LLM is an \u0027Information Generation system\u0027 (produces *new* tokens).",
        "motivation":  "The text makes this distinction very clear: \u0027A Search Engine... is an “Information Retrieval system”... finding **existing** information... An LLM is an Information Generation system. It produces **new** tokens out of previously seen ones.\u0027"
    },
    {
        "question":  "According to the text, what is the *wrong* way to use an LLM that leads to hallucinations?",
        "options":  [
                        "Asking it to brainstorm creative ideas",
                        "Asking it to rewrite an email in a professional tone",
                        "Asking it to summarize an article",
                        "Using it to retrieve facts, asking a \u0027generator tool\u0027 to do the job of a \u0027search tool\u0027"
                    ],
        "correct_answer":  "Using it to retrieve facts, asking a \u0027generator tool\u0027 to do the job of a \u0027search tool\u0027",
        "motivation":  "The text explicitly concludes its comparison of search engines and LLMs with this warning: \u0027**Using an LLM to retrieve facts is exactly what leads to hallucinations.** **You are asking a “generator tool” to do the job of a “search tool”.**\u0027"
    },
    {
        "question":  "What does the \u0027C\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Code",
                        "Creativity",
                        "Context",
                        "Clarity"
                    ],
        "correct_answer":  "Context",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027C\u0027 stands for Context, defined as the \u0027background information we need to understand the “universe” of any statement.\u0027"
    },
    {
        "question":  "What does the \u0027R\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Role",
                        "Reasoning",
                        "Retrieval",
                        "Response"
                    ],
        "correct_answer":  "Role",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027R\u0027 stands for Role, which \u0027heavily guides the model\u0027s tone and style.\u0027"
    },
    {
        "question":  "What does the \u0027O\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Output",
                        "Objective",
                        "Optimization",
                        "Options"
                    ],
        "correct_answer":  "Objective",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027O\u0027 stands for Objective, which means \u0027state the deliverable you expect, clearly and directly.\u0027"
    },
    {
        "question":  "What does the \u0027P\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Prompt",
                        "Product",
                        "Plausibility",
                        "Parameters"
                    ],
        "correct_answer":  "Parameters",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027P\u0027 stands for Parameters, which are \u0027additional **constraints**\u0027 that \u0027help deliver results more close to our request,\u0027 especially for coding."
    },
    {
        "question":  "In the \u0027bad prompt\u0027 example for the URL shortener, what was a key failure?",
        "options":  [
                        "The prompt was too short and did not provide enough detail",
                        "The prompt asked for a technical design plan instead of code",
                        "The prompt was a \u0027stream of consciousness\u0027 that lacked clear constraints and context (e.g., internal vs. public tool)",
                        "The prompt specified the wrong tech stack (Elixir and Phoenix)"
                    ],
        "correct_answer":  "The prompt was a \u0027stream of consciousness\u0027 that lacked clear constraints and context (e.g., internal vs. public tool)",
        "motivation":  "The text analyzes the \u0027bad prompt\u0027 as a \u0027long “stream of consciousness” prompt that *feels* detailed - but in reality is actually unfocused, lacks clear constraints... There’s no clear context: we’re not mentioning this shortener is an internal-use tool.\u0027"
    },
    {
        "question":  "In the \u0027good prompt\u0027 example for the URL shortener, what was a key *deliverable* requested from the LLM?",
        "options":  [
                        "All the Elixir and Phoenix code for the controller and Ecto schema",
                        "A comprehensive technical design plan, including routes, schema, and logic, but *no* Elixir or Phoenix code",
                        "A long \u0027stream of consciousness\u0027 brain-dump about URL shorteners",
                        "A list of alternative tech stacks to use instead of Elixir"
                    ],
        "correct_answer":  "A comprehensive technical design plan, including routes, schema, and logic, but *no* Elixir or Phoenix code",
        "motivation":  "The \u0027good prompt\u0027 explicitly instructs the LLM to \u0027Provide a comprehensive technical design plan\u0027 and, crucially, \u0027Do not write any Elixir or Phoenix code.\u0027 The goal was to define the system\u0027s components *before* implementation."
    },
    {
        "question":  "In the proposed software development framework, what is the \u0027Human Phase\u0027?",
        "options":  [
                        "The phase where the human blindly copies and pastes code from the LLM",
                        "The phase where the human provides the initial creative spark and the LLM does all the thinking and planning",
                        "The phase where the human *must think* and provide sketches, tech constraints, and a product description *before* asking the LLM to act",
                        "The phase where the human ranks the LLM\u0027s outputs for RLHF"
                    ],
        "correct_answer":  "The phase where the human *must think* and provide sketches, tech constraints, and a product description *before* asking the LLM to act",
        "motivation":  "The text emphasizes that to *properly* use LLMs, \u0027we must think, and quite a lot.\u0027 The \u0027Human Phase\u0027 involves the human expert doing upfront work: \u0027Research\u0027 (providing sketches/screenshots), defining \u0027Tech constraints,\u0027 and writing a \u0027Product (quick!) description.\u0027"
    },
    {
        "question":  "In the \u0027Agentic Phase\u0027 of software development, what is the \u0027Product Owner Agent\u0027 tasked with doing?",
        "options":  [
                        "Writing the final Elixir and Phoenix code",
                        "Creating a detailed multi-step implementation plan for the IDE Agent",
                        "Taking the human\u0027s sketches and requirements and expanding them into a full \u0027PRD document\u0027 (Product Requirements Document)",
                        "Finding bugs in the human\u0027s \u0027tech constraints\u0027 document"
                    ],
        "correct_answer":  "Taking the human\u0027s sketches and requirements and expanding them into a full \u0027PRD document\u0027 (Product Requirements Document)",
        "motivation":  "The text describes the \u0027Product Owner Agent\u0027 step as feeding the human-generated documents to a \u0027deep think model\u0027 and asking for a \u0027“product owner”-like document.\u0027 The output is a \u0027lengthy “PRD document” which explains our product, fully.\u0027"
    },
    {
        "question":  "In the \u0027Agentic Phase\u0027, why is it important for the \u0027Software Development Agent\u0027 to create a *plan* before writing code?",
        "options":  [
                        "This step is not important and can be skipped",
                        "To give the human something to read while the LLM writes the code",
                        "Because the agent needs to \u0027connect the dots\u0027 and understand *how* to apply the \u0027what\u0027 (the PRD) to the existing codebase",
                        "To generate a .txt file, which is the only format the LLM can write to"
                    ],
        "correct_answer":  "Because the agent needs to \u0027connect the dots\u0027 and understand *how* to apply the \u0027what\u0027 (the PRD) to the existing codebase",
        "motivation":  "The text stresses the importance of this planning step: \u0027We’ve fed the “what”, and this planning step will formulate a “how”. This enables the Agent to read the whole codebase, and now it’s able to “connect the dots”. This can’t be done (properly) in just one step!\u0027"
    },
    {
        "question":  "What is the final action the human is told to take after the \u0027Software Development Agent\u0027 creates its implementation plan?",
        "options":  [
                        "Immediately run the agent and let it code",
                        "Delete the plan and write the code yourself",
                        "**Read it.** And then edit or ask for changes *before* the agent implements them",
                        "Thank the agent and close the program"
                    ],
        "correct_answer":  "**Read it.** And then edit or ask for changes *before* the agent implements them",
        "motivation":  "The text is very explicit about this: \u0027Finally, we’ve asked your agent to save the output of the plan into an editable text file. **Read it.** Again, **don’t skip this step.** We can (finally) edit or ask for some changes *before* the agent implements those.\u0027"
    },
    {
        "question":  "What core concept from calculus is used to find the \u0027direction of steepest ascent\u0027 in a multi-dimensional function?",
        "options":  [
                        "Dot Product",
                        "Matrix",
                        "Derivative",
                        "Gradient (∇f)"
                    ],
        "correct_answer":  "Gradient (∇f)",
        "motivation":  "The text defines the derivative `f\u0027(x)` as the rate of change for a simple function. It then introduces the gradient `∇f` as \u0027the multi-dimensional version of a derivative\u0027 and states it \u0027is a vector that **points in the direction of the steepest ascent**.\u0027"
    },
    {
        "question":  "Why does the Stochastic Gradient Descent algorithm take a step in the *opposite* direction of the gradient?",
        "options":  [
                        "To maximize the error function",
                        "To find the direction of steepest ascent (go uphill)",
                        "To minimize the error (loss function) by going \u0027downhill\u0027 in the \u0027hilly landscape\u0027 of errors",
                        "To move to a random point in the landscape"
                    ],
        "correct_answer":  "To minimize the error (loss function) by going \u0027downhill\u0027 in the \u0027hilly landscape\u0027 of errors",
        "motivation":  "The text explains the goal is to \u0027minimize errors\u0027 by finding the \u0027lowest valley\u0027 in the \u0027hilly landscape\u0027 (the loss function). Since the gradient \u0027points uphill,\u0027 the algorithm takes \u0027a small step in **the opposite direction** (aka: go downhill)\u0027 to move towards the minimum."
    },
    {
        "question":  "What is the text\u0027s definition of a \u0027deterministic\u0027 system?",
        "options":  [
                        "A system with inherent randomness",
                        "A system where the same input can lead to different outputs",
                        "A system where the output is perfectly predictable from the input and involves no randomness",
                        "A machine learning model that is \u0027Probably Approximately Correct\u0027"
                    ],
        "correct_answer":  "A system where the output is perfectly predictable from the input and involves no randomness",
        "motivation":  "The text defines deterministic systems as: \u0027when the output *is perfectly predictable* from the input. No randomness is involved. The same input leads to the same output.\u0027"
    },
    {
        "question":  "What is the formal definition of the \u0027training data\u0027 in machine learning?",
        "options":  [
                        "The prediction rule `h: X→Y`",
                        "The set of all possible objects, `X`",
                        "A set of pairs of \u0027objects / labels\u0027, mathematically `S = {(x1,y1), ..., (xm, ym}`",
                        "The \u0027ground truth\u0027 function, `f`"
                    ],
        "correct_answer":  "A set of pairs of \u0027objects / labels\u0027, mathematically `S = {(x1,y1), ..., (xm, ym}`",
        "motivation":  "The text\u0027s \u0027complete definition\u0027 of learning lists \u0027A set of “training data”... mathematically: **`S = {(x1,y1), ..., (xm, ym}`**\u0027 which contains \u0027pairs of “objects / labels”\u0027."
    },
    {
        "question":  "What is the \u0027Mean Squared Error (MSE)\u0027 used for in the car dealership example?",
        "options":  [
                        "To measure the empirical error (training error) of the model",
                        "To set the initial random values for `weight` and `bias`",
                        "To calculate the gradient of the loss function",
                        "To define the \u0027small step\u0027 size in gradient descent"
                    ],
        "correct_answer":  "To measure the empirical error (training error) of the model",
        "motivation":  "The text states, \u0027To answer this question [How good is our computed model?], we need to measure its error on the data it was trained on. **That is the empirical error (or training error)**.\u0027 It then introduces Mean Squared Error (MSE) as \u0027A common way to measure it.\u0027"
    },
    {
        "question":  "Why was the 4th-degree polynomial model in the car example \u0027perfect\u0027 on the training data but bad in practice?",
        "options":  [
                        "It was too simple and \u0027underfit\u0027 the data",
                        "It was a Linear Regression model in disguise",
                        "It had \u0027overfit\u0027 the data, memorizing its random noise instead of the general trend, leading to poor generalization",
                        "It failed to achieve a training error of zero"
                    ],
        "correct_answer":  "It had \u0027overfit\u0027 the data, memorizing its random noise instead of the general trend, leading to poor generalization",
        "motivation":  "The text explains this as the \u0027overfitting trap.\u0027 The model \u0027memorized the training data, and adapted way too much, aka **including its random noise**, instead of “learning the general trend”.\u0027 This caused it to make unreasonable predictions (like a negative price) on new data."
    },
    {
        "question":  "What is the key takeaway from the \u0027overfitting trap\u0027 example?",
        "options":  [
                        "Always use the most complex model possible to get the training error to zero",
                        "A low training error is the primary and most important goal of machine learning",
                        "Linear models are always better than polynomial models",
                        "A low training error does not guarantee good performance on new data; the goal is to generalize, not memorize"
                    ],
        "correct_answer":  "A low training error does not guarantee good performance on new data; the goal is to generalize, not memorize",
        "motivation":  "The text concludes this section by stating, \u0027This demonstrates that having a low training error isn\u0027t our primary goal.\u0027 The real goal is to \u0027find a model `h` so that it can perform nicely in real world scenarios,\u0027 which is the definition of generalization."
    },
    {
        "question":  "In the formal definition of Reinforcement Learning, what is the \u0027State (S)\u0027?",
        "options":  [
                        "The agent\u0027s strategy or function",
                        "The reward the agent receives",
                        "A description of the environment at a specific moment",
                        "The action the agent chooses"
                    ],
        "correct_answer":  "A description of the environment at a specific moment",
        "motivation":  "The text\u0027s formal definitions for RL list \u0027State (S): A snapshot of the environment. E.g., the position of all pieces on a chessboard.\u0027"
    },
    {
        "question":  "What is an example of \u0027Incorrect or Inaccurate Data\u0027 provided in the text?",
        "options":  [
                        "A hiring dataset that is 90% male",
                        "Customer information where half the age entries are blank",
                        "A sensor recording a temperature of -200°C in Udine in July",
                        "A dataset that is split into training, validation, and test sets"
                    ],
        "correct_answer":  "A sensor recording a temperature of -200°C in Udine in July",
        "motivation":  "The text provides this as a specific example of \u0027Incorrect or Inaccurate Data\u0027: \u0027a sensor records a temperature of -200°C in Udine, in July.\u0027 The hiring data is \u0027Biased Data\u0027 and the blank age entries are \u0027Missing Data.\u0027"
    },
    {
        "question":  "What is the purpose of the \u0027Model Training\u0027 step in the core ML workflow?",
        "options":  [
                        "To clean, format, and transform the data",
                        "To gather raw data from various sources",
                        "To use the \u0027training set\u0027 to teach the model to find patterns and adjust its parameters",
                        "To deploy the model into a live production environment"
                    ],
        "correct_answer":  "To use the \u0027training set\u0027 to teach the model to find patterns and adjust its parameters",
        "motivation":  "The text describes the \u0027Model Training\u0027 step as the one where \u0027the model “learns”.\u0027 It specifies that \u0027The “training set” is used to actually learn the patterns and adjust the model’s parameters (like the `weight` and `bias`...)\u0027"
    },
    {
        "question":  "In the core ML workflow, what is the \u0027Deployment\u0027 step?",
        "options":  [
                        "The process of cleaning and formatting data",
                        "The final, one-time evaluation of the model on the test set",
                        "The process of integrating the trained model into a real-world application or system",
                        "The process of splitting data into K-Folds for cross-validation"
                    ],
        "correct_answer":  "The process of integrating the trained model into a real-world application or system",
        "motivation":  "The text lists \u0027Deployment\u0027 as step 4 of the core workflow, describing it as \u0027Putting the model into a production environment (e.g., inside an app, on a server) where it can make predictions on new, real-world data.\u0027"
    },
    {
        "question":  "What historical phase of LLMs is associated with RNNs (Recurrent Neural Networks) and LSTMs?",
        "options":  [
                        "Phase 1: Symbolic AI",
                        "Phase 2: Statistical",
                        "Phase 3: RNNs/LSTMs (Sequential)",
                        "Phase 4: The Transformer"
                    ],
        "correct_answer":  "Phase 3: RNNs/LSTMs (Sequential)",
        "motivation":  "The text\u0027s history of LLMs lists \u0027Phase 3: RNNs/LSTMs (Sequential)\u0027 and describes them as \u0027the first model types that could handle sequential data (like text) by using... a “memory” (or *state*).\u0027"
    },
    {
        "question":  "What was the main limitation of RNNs that the Transformer architecture aimed to solve?",
        "options":  [
                        "RNNs could not process text sequentially",
                        "RNNs\u0027 sequential, one-word-at-a-time processing and \u0027singleton memory\u0027 was a bottleneck",
                        "RNNs were \u0027plausibility engines,\u0027 not \u0027truth engines\u0027",
                        "RNNs were vulnerable to prompt injections"
                    ],
        "correct_answer":  "RNNs\u0027 sequential, one-word-at-a-time processing and \u0027singleton memory\u0027 was a bottleneck",
        "motivation":  "The text explains that the 2017 Transformer paper \u0027abandoned the previously adopted sequential approach of RNNs.\u0027 The Transformer\u0027s idea was \u0027instead of keeping a “singleton memory”, composed by reading input text one-word-at-a-time, to process and find the entire sentence context at once.\u0027"
    },
    {
        "question":  "What is \u0027Fine-Tuning\u0027 in the LLM architecture?",
        "options":  [
                        "The initial unsupervised training on web-scale data",
                        "The process of breaking text into numerical tokens",
                        "A *supervised* training step on a smaller, specific dataset to teach the model a particular task (e.g., summarization)",
                        "The process of using human feedback to make the model \u0027safer\u0027"
                    ],
        "correct_answer":  "A *supervised* training step on a smaller, specific dataset to teach the model a particular task (e.g., summarization)",
        "motivation":  "The text describes \u0027Fine-Tuning\u0027 as a step after pre-training. It is a \u0027supervised, training step\u0027 where the model is \u0027trained on a much smaller, *curated* dataset... for a *specific* task\u0027 (e.g., \u0027a dataset of questions and answers\u0027)."
    },
    {
        "question":  "According to the text, what is the myth about LLMs \u0027understanding\u0027 human language?",
        "options":  [
                        "LLMs truly understand the meaning and intent behind words, just like humans",
                        "LLMs \u0027understanding\u0027 is just a \u0027statistical map of which tokens are likely to follow other tokens\u0027 in a given context",
                        "LLMs\u0027 understanding comes from the \u0027Fine-Tuning\u0027 step, not the \u0027Pre-training\u0027 step",
                        "LLMs can only understand text, not code"
                    ],
        "correct_answer":  "LLMs \u0027understanding\u0027 is just a \u0027statistical map of which tokens are likely to follow other tokens\u0027 in a given context",
        "motivation":  "The text refutes the myth \u0027Can an LLM *understand* me?\u0027 by stating \u0027No.\u0027 It explains: \u0027It’s all about statistics... What we perceive as “understanding” is just an incredibly complex and massive statistical map of which tokens are likely to follow other tokens...\u0027"
    },
    {
        "question":  "What is the key difference between \u0027information retrieval\u0027 and \u0027information generation\u0027?",
        "options":  [
                        "Retrieval finds *existing* information, while generation produces *new* information (tokens)",
                        "Retrieval is what LLMs do, and generation is what search engines do",
                        "Retrieval is always factual, while generation is always fictional",
                        "There is no difference; they are two terms for the same process"
                    ],
        "correct_answer":  "Retrieval finds *existing* information, while generation produces *new* information (tokens)",
        "motivation":  "The text introduces these terms to contrast Search Engines and LLMs. A Search Engine (retrieval) is for \u0027finding **existing** information.\u0027 An LLM (generation) \u0027produces **new** tokens out of previously seen ones.\u0027"
    },
    {
        "question":  "In the C.R.O.P. framework, what is the purpose of the \u0027Role\u0027 (R) parameter?",
        "options":  [
                        "To provide the background information and context of the request",
                        "To state the clear, unambiguous deliverable that is expected",
                        "To add constraints, such as \u0027output as JSON\u0027",
                        "To guide the model\u0027s tone, style, and the knowledge-base it draws from (e.g., \u0027Act as a computer science professor\u0027)"
                    ],
        "correct_answer":  "To guide the model\u0027s tone, style, and the knowledge-base it draws from (e.g., \u0027Act as a computer science professor\u0027)",
        "motivation":  "The text explains the \u0027Role\u0027 parameter \u0027heavily guides the model\u0027s tone and style of the response.\u0027 It also suggests telling the LLM who *you* are to \u0027influence the “knowledge-base” it draws the attention from.\u0027"
    },
    {
        "question":  "What is a \u0027PRD document\u0027 as described in the \u0027Agentic Phase\u0027 of software development?",
        "options":  [
                        "A file containing the final, production-ready code",
                        "A document defining the \u0027core software engineering principles\u0027",
                        "A \u0027Product Requirements Document\u0027 that an LLM (acting as a Product Owner) expands from the human\u0027s initial sketches and notes",
                        "A text file containing the multi-step implementation plan from the \u0027Software Development Agent\u0027"
                    ],
        "correct_answer":  "A \u0027Product Requirements Document\u0027 that an LLM (acting as a Product Owner) expands from the human\u0027s initial sketches and notes",
        "motivation":  "The text describes the \u0027Product Owner Agent\u0027 step\u0027s goal as creating a \u0027PRD document.\u0027 This is a \u0027lengthy “PRD document” which explains our product, fully,\u0027 generated *from* the human\u0027s inputs (sketches, constraints, description)."
    },
    {
        "question":  "What does the text mean by \u0027anthropomorphize\u0027 in the context of LLMs?",
        "options":  [
                        "To treat LLMs as simple autocomplete tools",
                        "To use LLMs for information retrieval instead of generation",
                        "To attribute human-like qualities such as \u0027thinks,\u0027 \u0027knows,\u0027 or \u0027understands\u0027 to LLMs",
                        "To use the C.R.O.P. framework for prompting"
                    ],
        "correct_answer":  "To attribute human-like qualities such as \u0027thinks,\u0027 \u0027knows,\u0027 or \u0027understands\u0027 to LLMs",
        "motivation":  "The text states, \u0027We, humans, are quite the egocentric beings. We tend to *anthropomorphize* everything. LLMs are no exception: we use wordings like “it thinks”, “it says”, “it lied” or “it understands me”.\u0027"
    },
    {
        "question":  "In the \u0027Human Phase\u0027 of software development with LLMs, what is the \u0027Tech constraints\u0027 output?",
        "options":  [
                        "A full architecture document for the application, written by the human",
                        "A small \u0027structure document\u0027 explaining the human\u0027s \u0027core software engineering principles\u0027 (e.g., frameworks, libraries)",
                        "Sketches and screenshots of the desired result",
                        "A lengthy \u0027PRD document\u0027 written by the human"
                    ],
        "correct_answer":  "A small \u0027structure document\u0027 explaining the human\u0027s \u0027core software engineering principles\u0027 (e.g., frameworks, libraries)",
        "motivation":  "The text defines the \u0027Tech constraints\u0027 step as the human putting on the \u0027architect hat\u0027 to define \u0027languages, frameworks, libraries, core software principles.\u0027 The output is \u0027a small “structure document” explaining our “core software engineering principles”.\u0027"
    },
    {
        "question":  "What is a dot product, as described in the text?",
        "options":  [
                        "A grid of numbers representing a dataset",
                        "The multi-dimensional version of a derivative",
                        "An operation that tells how aligned two vectors are, calculated by multiplying corresponding elements and summing the results",
                        "A cluster of common information describing a single data point"
                    ],
        "correct_answer":  "An operation that tells how aligned two vectors are, calculated by multiplying corresponding elements and summing the results",
        "motivation":  "The text defines a dot product as \u0027a fundamental operation that tells us *how aligned two vectors are*.\u0027 It provides an example, \u0027A⋅B = 1*3 + 2*4 = 11\u0027, which shows the process of multiplying corresponding elements and summing them."
    },
    {
        "question":  "What does a dot product value near zero imply?",
        "options":  [
                        "The two vectors point in a similar direction",
                        "The two vectors point in opposite directions",
                        "The two vectors are unrelated (geometrically perpendicular)",
                        "The two vectors are identical"
                    ],
        "correct_answer":  "The two vectors are unrelated (geometrically perpendicular)",
        "motivation":  "The text states: \u0027a value near zero means two vectors are geometrically perpendicular, aka **they are unrelated**.\u0027"
    },
    {
        "question":  "What is the text\u0027s view on the Elixir programming language?",
        "options":  [
                        "It is a complex language that is not suitable for machine learning",
                        "It is a language that relies heavily on \u0027mutability\u0027 and classical \u0027loops\u0027",
                        "It is a functional programming language that has no \u0027mutability\u0027 and uses recursion, which will be used for ML exercises",
                        "It is a language primarily used for statistical analysis and calculus"
                    ],
        "correct_answer":  "It is a functional programming language that has no \u0027mutability\u0027 and uses recursion, which will be used for ML exercises",
        "motivation":  "The text introduces Elixir as a \u0027functional programming language\u0027 that will \u0027enable us to do some hands-on machine learning exercises!\u0027 It specifically notes that \u0027Elixir has no *mutability*\u0027 and \u0027you won’t find “loops” in a classical sense: only recursion is accepted.\u0027"
    },
    {
        "question":  "What is the formal definition of \u0027ML\u0027 provided in the text?",
        "options":  [
                        "The science of getting computers to act without being *explicitly* programmed",
                        "A method for finding a \u0027ground truth\u0027 function `f` that is perfectly accurate",
                        "A system for writing \u0027dumb by-hand rules\u0027 to classify data",
                        "The process of using a search engine to retrieve information"
                    ],
        "correct_answer":  "The science of getting computers to act without being *explicitly* programmed",
        "motivation":  "The text in \u0027Machine Learning 101\u0027 provides this exact definition: \u0027Machine Learning (ML) is the science of getting computers to act without being *explicitly* programmed.\u0027"
    },
    {
        "question":  "In the \u0027ML in a nutshell\u0027 recipe, what is the third step?",
        "options":  [
                        "Fetch some data, call it \u0027training set\u0027",
                        "Define a model, define its parameters, and run it on the training set",
                        "Tune the parameters, so that it minimizes the empirical error on the training set",
                        "Deploy the model to a production environment"
                    ],
        "correct_answer":  "Tune the parameters, so that it minimizes the empirical error on the training set",
        "motivation":  "The text summarizes \u0027Machine Learning, in a nutshell\u0027 in three steps. The third step is: \u0027Tune the parameters, so that it minimizes the empirical error on the training set.\u0027"
    },
    {
        "question":  "In the car dealership example, what is the \u0027bias\u0027 in the linear model `predicted_price = weight * mileage + bias`?",
        "options":  [
                        "The parameter that controls the relationship between mileage and price",
                        "The empirical error of the model",
                        "The starting price of a car, or the predicted price when the mileage is zero",
                        "The \u0027small step\u0027 taken during gradient descent"
                    ],
        "correct_answer":  "The starting price of a car, or the predicted price when the mileage is zero",
        "motivation":  "In a linear model `y = mx + b`, the \u0027b\u0027 term (here, \u0027bias\u0027) is the y-intercept, which is the value of `y` when `x` is 0. In this context, it represents the predicted price (`y`) when the mileage (`x`) is 0."
    },
    {
        "question":  "In the car dealership example, what is the \u0027small step\u0027 (or learning rate) used for?",
        "options":  [
                        "It is the final `weight` parameter of the trained model",
                        "It is the `bias` parameter of the trained model",
                        "It controls how much the `weight` and `bias` are adjusted during each iteration of gradient descent",
                        "It is the Mean Squared Error of the model"
                    ],
        "correct_answer":  "It controls how much the `weight` and `bias` are adjusted during each iteration of gradient descent",
        "motivation":  "The text describes \u0027Step 3\u0027 of the gradient descent example as choosing a \u0027“small step” to be `0.001`.\u0027 It\u0027s crucial because \u0027Too big, and we leap too far. Too small, and we won’t ever reach the minimum error.\u0027 This step value is used in \u0027Step 4\u0027 to update the parameters."
    },
    {
        "question":  "What is the key difference between Supervised and Unsupervised Learning?",
        "options":  [
                        "Supervised learning uses a \u0027reward\u0027 system, while Unsupervised learning does not",
                        "Supervised learning data is \u0027labeled\u0027 (has inputs and correct outputs), while Unsupervised learning data is \u0027unlabeled\u0027",
                        "Supervised learning finds hidden patterns, while Unsupervised learning learns a mapping function",
                        "Supervised learning is used for LLMs, while Unsupervised learning is used for linear regression"
                    ],
        "correct_answer":  "Supervised learning data is \u0027labeled\u0027 (has inputs and correct outputs), while Unsupervised learning data is \u0027unlabeled\u0027",
        "motivation":  "The text draws a clear line: \u0027Supervised Learning: When the data is labeled. Goal: To learn a mapping function...\u0027. In contrast: \u0027Unsupervised Learning: The data is unlabeled. Goal: To find hidden structures and patterns...\u0027"
    },
    {
        "question":  "In the Reinforcement Learning analogy, who is the \u0027Agent\u0027?",
        "options":  [
                        "The human player",
                        "The set of rules for the game",
                        "The \u0027world\u0027 or \u0027game board\u0027 itself",
                        "The reward or \u0027Good boy!\u0027"
                    ],
        "correct_answer":  "The human player",
        "motivation":  "The text provides an analogy for RL: \u0027You (Agent) are playing a video game (Environment). You press a button (Action) while at a certain screen (State). The game gives you points (Reward)...\u0027"
    },
    {
        "question":  "In the core ML workflow, what is the \u0027Monitoring \u0026 Maintenance\u0027 step?",
        "options":  [
                        "The initial gathering of raw data",
                        "The process of cleaning and formatting data",
                        "The one-time deployment of the model",
                        "The ongoing process of tracking the model\u0027s performance in production and retraining it as needed"
                    ],
        "correct_answer":  "The ongoing process of tracking the model\u0027s performance in production and retraining it as needed",
        "motivation":  "The text lists \u0027Monitoring \u0026 Maintenance\u0027 as the final step in the workflow: \u0027Tracking the model’s performance in the real world. Over time, data can change... requiring the model to be retrained or updated.\u0027"
    },
    {
        "question":  "What is the \u0027Transformer\u0027 in the context of LLMs?",
        "options":  [
                        "An early form of Symbolic AI from the 1960s",
                        "A type of RNN that uses a \u0027singleton memory\u0027",
                        "The 2017 architecture introduced in \u0027Attention Is All You Need\u0027 that processes sentence context all at once using self-attention",
                        "The process of cleaning and transforming data before training"
                    ],
        "correct_answer":  "The 2017 architecture introduced in \u0027Attention Is All You Need\u0027 that processes sentence context all at once using self-attention",
        "motivation":  "The text identifies the \u0027Transformer\u0027 as the \u0027new architecture\u0027 from the 2017 paper \u0027Attention Is All You Need.\u0027 Its core idea is \u0027to process and find the entire sentence context at once\u0027 using \u0027self-attention.\u0027"
    },
    {
        "question":  "What is the \u0027plausibility engine\u0027 vs \u0027truth engine\u0027 distinction meant to explain?",
        "options":  [
                        "Why LLMs are so accurate at retrieving facts",
                        "Why LLMs \u0027hallucinate\u0027 (they predict the next plausible token, not the next *true* token)",
                        "Why LLMs are vulnerable to prompt injections",
                        "Why LLMs have a knowledge cutoff"
                    ],
        "correct_answer":  "Why LLMs \u0027hallucinate\u0027 (they predict the next plausible token, not the next *true* token)",
        "motivation":  "The text uses this distinction to explain hallucinations: \u0027The LLM is not “lying”... its pre-trained model goal is to predict the next plausible token in a sequence... It is a “plausibility engine”, not a “truth engine”.\u0027"
    },
    {
        "question":  "What is the text\u0027s advice for a human developer *before* using an LLM in the \u0027Agentic Phase\u0027?",
        "options":  [
                        "To ask the LLM for a plan, then blindly trust it",
                        "To do the \u0027Human Phase\u0027 work first: provide sketches, tech constraints, and a product description",
                        "To write a long \u0027stream of consciousness\u0027 prompt with all your ideas",
                        "To use the LLM as a search engine to find the right code"
                    ],
        "correct_answer":  "To do the \u0027Human Phase\u0027 work first: provide sketches, tech constraints, and a product description",
        "motivation":  "The text introduces the \u0027Agentic Phase\u0027 as something that happens *after* the \u0027Human Phase.\u0027 The \u0027Human Phase\u0027 requires the human to \u0027provide a simple sketch,\u0027 define \u0027Tech constraints,\u0027 and write a \u0027Product (quick!) description\u0027 *before* the agent is asked to do work."
    },
    {
        "question":  "According to the text, what is a vector?",
        "options":  [
                        "A grid of numbers, like a spreadsheet",
                        "A cluster of common information that describes a single point of data",
                        "A mathematical operation that tells how aligned two points are",
                        "The rate of change of a function at a given point"
                    ],
        "correct_answer":  "A cluster of common information that describes a single point of data",
        "motivation":  "The source material defines a vector as a \u0027cluster of common information that describes a single point of data,\u0027 using the example of a student\u0027s attributes [19, 12, 88]. A grid of numbers is a matrix, an operation for alignment is a dot product, and the rate of change is a derivative."
    },
    {
        "question":  "What does a large, positive dot product between two vectors indicate?",
        "options":  [
                        "The vectors are unrelated and geometrically perpendicular",
                        "The vectors point in opposite directions and contrast each other",
                        "The vectors point in a similar direction",
                        "The vectors represent a spreadsheet of data"
                    ],
        "correct_answer":  "The vectors point in a similar direction",
        "motivation":  "The text states, \u0027a large, positive, dot product means the vectors point in a similar direction.\u0027 A value near zero means they are unrelated (perpendicular), and a large negative value means they point in opposite directions."
    },
    {
        "question":  "What is a matrix defined as in the source material?",
        "options":  [
                        "A list of numbers describing a single student",
                        "A fundamental operation that tells us how aligned two vectors are",
                        "A grid of numbers, like a spreadsheet, often representing a collection of data points",
                        "The multi-dimensional version of a derivative"
                    ],
        "correct_answer":  "A grid of numbers, like a spreadsheet, often representing a collection of data points",
        "motivation":  "The text explains that if you stack multiple vectors (like data for 100 students) together, you get a grid of numbers, which \u0027in math, is called a matrix.\u0027 It is compared to a spreadsheet."
    },
    {
        "question":  "In the context of calculus for ML, what does a derivative, `f\u0027(x)`, tell you?",
        "options":  [
                        "The direction of the steepest ascent for a multi-dimensional function",
                        "The average value of a dataset",
                        "The rate of change or \u0027trend\u0027 of a function at any given point",
                        "How aligned two vectors are"
                    ],
        "correct_answer":  "The rate of change or \u0027trend\u0027 of a function at any given point",
        "motivation":  "The source material defines the derivative `f\u0027(x)` as telling you the function\u0027s \u0027rate of change at any given point (geometrically, that’s simply its *slope*).\u0027 This helps understand the function\u0027s \u0027trend\u0027."
    },
    {
        "question":  "What is the gradient (`∇f`) described as in the text?",
        "options":  [
                        "A function used to model errors as a \u0027hilly landscape\u0027",
                        "The multi-dimensional version of a derivative",
                        "A collection of data points in a grid",
                        "A process that starts at a random point and steps downhill"
                    ],
        "correct_answer":  "The multi-dimensional version of a derivative",
        "motivation":  "The text explicitly states that the gradient, `∇f`, \u0027is the multi-dimensional version of a derivative.\u0027 It points in the direction of the steepest ascent."
    },
    {
        "question":  "In machine learning, what is the primary goal of optimization?",
        "options":  [
                        "To find the average and spread of the data",
                        "To stack vectors into a matrix",
                        "To minimize errors and maximize results",
                        "To ensure a system is deterministic"
                    ],
        "correct_answer":  "To minimize errors and maximize results",
        "motivation":  "The text describes optimization using the \u0027min-maxing\u0027 concept: \u0027We want to minimize the errors, *and* maximize results.\u0027 This is a core problem that optimization as a field addresses."
    },
    {
        "question":  "What is a \u0027loss function\u0027 in the context of ML optimization?",
        "options":  [
                        "A function that models the \u0027hilly landscape\u0027 of errors",
                        "A function that calculates the dot product of two vectors",
                        "The algorithm used to find the minimum error",
                        "A function that represents the \u0027ground truth\u0027"
                    ],
        "correct_answer":  "A function that models the \u0027hilly landscape\u0027 of errors",
        "motivation":  "The text explains that to find the lowest valley (minimum error), \u0027we could model our errors as a function of that “hilly landscape”; we call such function a *loss function*.\u0027"
    },
    {
        "question":  "What is the core idea of the Stochastic Gradient Descent algorithm?",
        "options":  [
                        "To analytically compute the exact optimal solution",
                        "To take a small step in the direction of the gradient (uphill)",
                        "To start at a random point, calculate the gradient, and take a small step in the opposite direction (downhill)",
                        "To perfectly fit a model to every single data point in the training set"
                    ],
        "correct_answer":  "To start at a random point, calculate the gradient, and take a small step in the opposite direction (downhill)",
        "motivation":  "The text outlines the Stochastic Gradient Descent algorithm as: 1. start at a random point, 2. calculate the gradient, and 3. take a small step in the *opposite* direction (downhill). This is repeated iteratively."
    },
    {
        "question":  "Which statistical measure is described as \u0027robust to outliers\u0027?",
        "options":  [
                        "Mean",
                        "Median",
                        "Variance",
                        "Standard Deviation"
                    ],
        "correct_answer":  "Median",
        "motivation":  "The source material defines the \u0027Median\u0027 as \u0027The middle value when the data is sorted. More robust to outliers.\u0027 The Mean, in contrast, is \u0027Prone to being skewed by outliers.\u0027"
    },
    {
        "question":  "What is the key characteristic of a stochastic system?",
        "options":  [
                        "The output is perfectly predictable from the input",
                        "The same input always leads to the same output",
                        "There is inherent randomness, and the same input can lead to different outputs",
                        "It represents the \u0027ground truth\u0027 mapping"
                    ],
        "correct_answer":  "There is inherent randomness, and the same input can lead to different outputs",
        "motivation":  "The text defines stochastic systems as having \u0027inherent randomness\u0027 where the \u0027same input can lead to different outputs.\u0027 In contrast, deterministic systems are perfectly predictable."
    },
    {
        "question":  "What core feature of Elixir programming is highlighted in the text?",
        "options":  [
                        "It uses classical \u0027loops\u0027 for iteration",
                        "It has no mutability, meaning variables can only be reassigned, not changed",
                        "It is primarily used for statistical analysis",
                        "It is a deterministic systems language"
                    ],
        "correct_answer":  "It has no mutability, meaning variables can only be reassigned, not changed",
        "motivation":  "The source states, \u0027Elixir has no *mutability*, meaning: you can’t really *change* variables, only *reassign* them.\u0027 It also notes that this is why it relies on recursion instead of classical loops."
    },
    {
        "question":  "What is the \u0027paradigm shift\u0027 of Machine Learning compared to Traditional Programming?",
        "options":  [
                        "In ML, the programmer figures out all the rules explicitly",
                        "In ML, the computer is provided with data and figures out the rules on its own",
                        "In ML, the goal is to create deterministic systems",
                        "In ML, programs are written in functional languages like Elixir"
                    ],
        "correct_answer":  "In ML, the computer is provided with data and figures out the rules on its own",
        "motivation":  "The text contrasts Traditional Programming, where \u0027The programmer figures out the rules,\u0027 with Machine Learning, where \u0027we provide the computer with data... The computer then figures out the rules on its own.\u0027"
    },
    {
        "question":  "What does \u0027PAC\u0027 stand for in the context of ML models?",
        "options":  [
                        "Programming and Calculus",
                        "Pattern, Algorithm, Compute",
                        "Probably Approximately Correct",
                        "Prediction, Accuracy, Classification"
                    ],
        "correct_answer":  "Probably Approximately Correct",
        "motivation":  "The text introduces \u0027Probably Approximately Correct (PAC) models\u0027 as algorithms that \u0027work with high probability\u0027 and whose output \u0027might not be optimal,\u0027 combining \u0027approximately correct\u0027 heuristics with \u0027randomized\u0027 steps."
    },
    {
        "question":  "In the formal definition of machine learning, what is the \u0027prediction rule\u0027 or \u0027model\u0027?",
        "options":  [
                        "The set of all possible objects, `X`",
                        "The set of all possible labels, `Y`",
                        "The set of training data, `S`",
                        "A function `h: X→Y` that predicts a label for a given object"
                    ],
        "correct_answer":  "A function `h: X→Y` that predicts a label for a given object",
        "motivation":  "The text states, \u0027A learner output is a prediction rule... a function that, given an object, tries to *predict* a... label.\u0027 It is mathematically written as `h: X→Y` and is called the ML model."
    },
    {
        "question":  "What is the \u0027empirical error\u0027?",
        "options":  [
                        "The \u0027ground truth\u0027 error, which is impossible to compute",
                        "The error of the prediction rule `h` computed on the training set `S`",
                        "An error in the Elixir code",
                        "The error caused by using a linear model instead of a polynomial one"
                    ],
        "correct_answer":  "The error of the prediction rule `h` computed on the training set `S`",
        "motivation":  "The text explains that since we can\u0027t compute the \u0027perfect minimal error\u0027 (using the unknown `f`), we instead \u0027compute the error of `h` based on our training set `S`.\u0027 This is the empirical error."
    },
    {
        "question":  "In the car dealership example, what model was proposed as the *simplest* option?",
        "options":  [
                        "A high-degree polynomial model",
                        "A stochastic gradient descent model",
                        "A neural network model",
                        "A Linear Regression model"
                    ],
        "correct_answer":  "A Linear Regression model",
        "motivation":  "The text says, \u0027Let’s just propose the simplest model we can think of: Linear Regression.\u0027 The model\u0027s hypothesis is given as a straight line: `predicted_price = weight * mileage + bias`."
    },
    {
        "question":  "What is \u0027overfitting\u0027?",
        "options":  [
                        "A model that is too simple to capture the underlying trend",
                        "The process of minimizing the Mean Squared Error (MSE)",
                        "When a model \u0027memorized\u0027 the training data, including its random noise, and fails to generalize",
                        "The use of a Linear Regression model for a complex problem"
                    ],
        "correct_answer":  "When a model \u0027memorized\u0027 the training data, including its random noise, and fails to generalize",
        "motivation":  "The text describes overfitting as when the model \u0027memorized the training data, and adapted way too much, aka including its random noise, instead of “learning the general trend”.\u0027 This leads to poor performance on new data."
    },
    {
        "question":  "What is the primary characteristic of Supervised Learning?",
        "options":  [
                        "The data is unlabeled, and the machine must find hidden structures",
                        "The machine learns by interacting with an environment and receiving rewards or penalties",
                        "The data is labeled, and the goal is to learn a mapping function from inputs to outputs",
                        "The machine uses a \u0027plausibility engine\u0027 to predict the next token"
                    ],
        "correct_answer":  "The data is labeled, and the goal is to learn a mapping function from inputs to outputs",
        "motivation":  "The source defines Supervised Learning as the case \u0027When the data is labeled,\u0027 and states its goal is \u0027To learn a mapping function from inputs (X) to outputs (Y).\u0027"
    },
    {
        "question":  "What is the core task of \u0027Unsupervised Learning\u0027?",
        "options":  [
                        "To predict a continuous value, like a house price",
                        "To learn from labeled \u0027ground truth\u0027 data",
                        "To find hidden structures or patterns in unlabeled data",
                        "To learn a policy through trial and error with rewards"
                    ],
        "correct_answer":  "To find hidden structures or patterns in unlabeled data",
        "motivation":  "The text states that in Unsupervised Learning, \u0027The data is unlabeled\u0027 and the \u0027Goal: To find hidden structures and patterns in data.\u0027 This is contrasted with Supervised Learning, which uses labeled data."
    },
    {
        "question":  "How is \u0027Reinforcement Learning\u0027 (RL) described?",
        "options":  [
                        "As a method for finding patterns in unlabeled data",
                        "As a method for learning a mapping from inputs (X) to outputs (Y)",
                        "As an \u0027agent\u0027 learning through trial and error by interacting with an environment to maximize a \u0027reward\u0027",
                        "As a system that is perfectly predictable and involves no randomness"
                    ],
        "correct_answer":  "As an \u0027agent\u0027 learning through trial and error by interacting with an environment to maximize a \u0027reward\u0027",
        "motivation":  "The text describes Reinforcement Learning as an \u0027Agent\u0027 that \u0027interacts with an Environment.\u0027 It \u0027learns by trial and error\u0027 based on \u0027Rewards\u0027 or \u0027Penalties\u0027 with the \u0027Goal: To learn the best “policy”... that maximizes its cumulative reward over time.\u0027"
    },
    {
        "question":  "What is the \u0027garbage in, garbage out\u0027 (GIGO) principle in ML?",
        "options":  [
                        "The idea that ML models can fix bad data through optimization",
                        "The fact that the quality of an ML model\u0027s output is entirely dependent on the quality of its input data",
                        "A type of model used for sorting waste",
                        "The process of cleaning and preprocessing data"
                    ],
        "correct_answer":  "The fact that the quality of an ML model\u0027s output is entirely dependent on the quality of its input data",
        "motivation":  "The text introduces \u0027Garbage in, Garbage out\u0027 to mean \u0027the quality of your data *directly* determines the quality of your model.\u0027 If the input data is flawed, the model\u0027s predictions will be, too."
    },
    {
        "question":  "What is an example of \u0027Biased Data\u0027 given in the text?",
        "options":  [
                        "A sensor recording an impossible temperature",
                        "A customer dataset where half the age entries are blank",
                        "A hiring system trained on data where 90% of past hires were male, leading the model to favor male candidates",
                        "Including a student\u0027s \u0027shoe size\u0027 to predict their exam score"
                    ],
        "correct_answer":  "A hiring system trained on data where 90% of past hires were male, leading the model to favor male candidates",
        "motivation":  "The text provides this exact example for Biased Data: \u0027a hiring ML system uses data... where, historically, 90% of engineers hired were male: the model will likely learn that being male is a key characteristic... and will be biased against qualified female candidates.\u0027"
    },
    {
        "question":  "In the core ML workflow, what is \u0027Preprocessing\u0027?",
        "options":  [
                        "The final step where the model is put into a production environment",
                        "The step where the model \u0027learns\u0027 the patterns from data",
                        "Gathering the raw data from databases, APIs, or files",
                        "Cleaning, formatting, and transforming data into a usable state"
                    ],
        "correct_answer":  "Cleaning, formatting, and transforming data into a usable state",
        "motivation":  "The source material\u0027s \u0027core workflow\u0027 lists Preprocessing as the step that \u0027involves cleaning the data (handling missing values, removing duplicates), formatting it, and transforming it into a usable state.\u0027"
    },
    {
        "question":  "What is the purpose of the \u0027validation set\u0027 in a training framework?",
        "options":  [
                        "It is the main dataset used to \u0027learn\u0027 the model parameters (like weights)",
                        "It is used for the final, one-time evaluation of the *best* model\u0027s performance",
                        "It is used to compare different models or tune hyperparameters, to see which model generalizes best",
                        "It is a dataset of \u0027ground truth\u0027 that is never used"
                    ],
        "correct_answer":  "It is used to compare different models or tune hyperparameters, to see which model generalizes best",
        "motivation":  "The text explains the validation set is used \u0027to *validate* which one [model] we should pick\u0027 and to tune \u0027hyperparameters.\u0027 It\u0027s for model selection, unlike the training set (for learning) or the test set (for final evaluation)."
    },
    {
        "question":  "What is the \u0027test set\u0027 used for?",
        "options":  [
                        "To train the model on the largest portion of data",
                        "To compare different models against each other to pick the best one",
                        "To provide a final, unbiased assessment of the *chosen* model\u0027s performance on unseen data",
                        "To find hidden patterns in unlabeled data"
                    ],
        "correct_answer":  "To provide a final, unbiased assessment of the *chosen* model\u0027s performance on unseen data",
        "motivation":  "The text states the test set is \u0027a final, held-back, *untouched* dataset.\u0027 It \u0027is used only *once*\u0027 to \u0027get a final, unbiased measure of how our *chosen* model... will perform in the real world.\u0027"
    },
    {
        "question":  "In the car dealership example, what is the \u0027weight\u0027 in the linear model `predicted_price = weight * mileage + bias`?",
        "options":  [
                        "The starting price of all cars",
                        "A parameter that represents how much the price changes for each additional km of mileage",
                        "The average mileage of all cars in the dataset",
                        "The final predicted price of the car"
                    ],
        "correct_answer":  "A parameter that represents how much the price changes for each additional km of mileage",
        "motivation":  "In the linear regression model `predicted_price = weight * mileage + bias`, the \u0027weight\u0027 (or slope) determines the relationship between mileage and price. A negative weight, as in the example `-0.14`, means the price decreases by that amount for each unit increase in mileage."
    },
    {
        "question":  "What does the text identify as the main goal of a machine learning model, which differs from having a low training error?",
        "options":  [
                        "To achieve a training error of exactly zero",
                        "To create the most complex model possible, like a high-degree polynomial",
                        "To find a model `h` that performs well in real-world scenarios on new, unseen data",
                        "To perfectly memorize all the data points in the training set"
                    ],
        "correct_answer":  "To find a model `h` that performs well in real-world scenarios on new, unseen data",
        "motivation":  "The text explicitly states that \u0027having a low training error isn\u0027t our primary goal.\u0027 The overfitting example demonstrates that a zero-error model can be useless. The true goal is to \u0027find a model `h` so that it can perform nicely in real world scenarios\u0027 (generalization)."
    },
    {
        "question":  "In the formal definition of Reinforcement Learning, what is a \u0027policy\u0027?",
        "options":  [
                        "The reward or penalty the agent receives",
                        "The agent\u0027s strategy for choosing an action based on the current state",
                        "The set of all possible states in the environment",
                        "The environment the agent interacts with"
                    ],
        "correct_answer":  "The agent\u0027s strategy for choosing an action based on the current state",
        "motivation":  "The text defines the \u0027Policy (π)\u0027 in Reinforcement Learning as the \u0027Agent’s strategy. This is a function that maps a state (S) to an action (A).\u0027 The agent\u0027s goal is to learn the *best* policy."
    },
    {
        "question":  "What is an example of \u0027Missing Data\u0027 provided in the text?",
        "options":  [
                        "A sensor recording -200°C in July",
                        "A hiring dataset that is 90% male",
                        "Customer information where half the entries for age are blank",
                        "A house listed with 10 square meters and 15 bedrooms"
                    ],
        "correct_answer":  "Customer information where half the entries for age are blank",
        "motivation":  "The text gives a specific example of Missing Data: \u0027customer information where half the entries for age are blank, while being interested in its purchasing habits.\u0027 The sensor and house examples relate to \u0027Incorrect or Inaccurate Data,\u0027 and the hiring example relates to \u0027Biased Data.\u0027"
    },
    {
        "question":  "What is the purpose of \u0027K-Fold Cross-Validation\u0027?",
        "options":  [
                        "To deploy the model to production",
                        "To gather the initial raw data",
                        "To get a more robust estimate of model performance and avoid bias from a single train/validation split",
                        "To clean and format the data before training"
                    ],
        "correct_answer":  "To get a more robust estimate of model performance and avoid bias from a single train/validation split",
        "motivation":  "The text explains that K-Fold Cross-Validation is used \u0027to get a more robust estimate\u0027 of a model\u0027s performance. It involves splitting the data into \u0027K\u0027 folds and training/evaluating the model \u0027K\u0027 times, each time using a different fold as the validation set, then averaging the results."
    },
    {
        "question":  "What 2017 Google paper introduced the \u0027Transformer\u0027 architecture, changing the field of LLMs?",
        "options":  [
                        "\"The Unreasonable Effectiveness of Recurrent Neural Networks\"",
                        "\"Attention Is All You Need\"",
                        "\"Deep Learning for Natural Language Processing\"",
                        "\"Reinforcement Learning from Human Feedback\""
                    ],
        "correct_answer":  "\"Attention Is All You Need\"",
        "motivation":  "The text identifies Phase 4 of LLM history as starting in 2017 with the Google paper titled \u0027\"Attention Is All You Need\"\u0027. This paper \u0027introduced a new architecture: the Transformer.\u0027"
    },
    {
        "question":  "What is the core idea of \u0027self-attention\u0027 in the Transformer architecture?",
        "options":  [
                        "To process text one word at a time, keeping a \u0027singleton memory\u0027",
                        "To break text down into numerical tokens",
                        "To figure out which words in a sentence are most important to which other words, processing the context all at once",
                        "To fine-tune the model on a specific, supervised task"
                    ],
        "correct_answer":  "To figure out which words in a sentence are most important to which other words, processing the context all at once",
        "motivation":  "The text explains that the Transformer\u0027s core idea was to \u0027process and find the entire sentence context at once.\u0027 The mechanism for this is \u0027self-attention,\u0027 and its \u0027job is to figure out which words are most important to which other words.\u0027"
    },
    {
        "question":  "In an LLM architecture, what is \u0027Tokenization\u0027?",
        "options":  [
                        "Looking up a token in a giant dictionary to get a \u0027Meaning Vector\u0027",
                        "Training the model to predict the next token in a sequence",
                        "The process of breaking text down into numbers (tokens) that the model can read",
                        "Using human feedback to rank the model\u0027s outputs"
                    ],
        "correct_answer":  "The process of breaking text down into numbers (tokens) that the model can read",
        "motivation":  "The text describes \u0027Tokenization\u0027 as the first step in the LLM architecture: \u0027the model can\u0027t read “words”... We need to break text down into numbers. These are called “tokens”.\u0027"
    },
    {
        "question":  "What is an \u0027Embedding\u0027 in the context of LLMs?",
        "options":  [
                        "The final output of the model after fine-tuning",
                        "The process of breaking text into tokens",
                        "A vector that represents a token\u0027s \u0027position\u0027 in a multi-dimensional \u0027concept space\u0027",
                        "A security vulnerability where a user tricks the model"
                    ],
        "correct_answer":  "A vector that represents a token\u0027s \u0027position\u0027 in a multi-dimensional \u0027concept space\u0027",
        "motivation":  "The text defines an \u0027embedding\u0027 as the step after tokenization. The model looks up each token in a table that \u0027simply contains *a vector* for each token. This vector represents the token\u0027s \"position\" in a multi-dimensional “concept space”.\u0027"
    },
    {
        "question":  "What is \u0027Pre-training\u0027 in the LLM architecture?",
        "options":  [
                        "The final step where human feedback is used to refine the model\u0027s helpfulness",
                        "The main *unsupervised* learning phase where the model learns by predicting the next token in a massive dataset",
                        "The *supervised* learning phase where the model is trained on specific tasks like translation or summarization",
                        "The process of creating \u0027Meaning Vectors\u0027 (embeddings) for tokens"
                    ],
        "correct_answer":  "The main *unsupervised* learning phase where the model learns by predicting the next token in a massive dataset",
        "motivation":  "The text describes \u0027Pre-training\u0027 as \u0027the biggest, unsupervised, training step\u0027 where the model \u0027is fed *all* the data... Its only goal is to predict the next token.\u0027 This is contrasted with \u0027Fine-Tuning,\u0027 which is supervised."
    },
    {
        "question":  "What is \u0027RLHF\u0027 (Reinforcement Learning from Human Feedback)?",
        "options":  [
                        "The initial unsupervised training step on web-scale data",
                        "The process of breaking text into numerical tokens",
                        "A 2017 paper that introduced the Transformer architecture",
                        "A fine-tuning step where human rankings of model outputs are used to train a \u0027reward model\u0027 to make the LLM more helpful and aligned"
                    ],
        "correct_answer":  "A fine-tuning step where human rankings of model outputs are used to train a \u0027reward model\u0027 to make the LLM more helpful and aligned",
        "motivation":  "The text explains \u0027RLHF\u0027 as a step after \u0027Fine-Tuning.\u0027 It involves \u0027humans... ranking the outputs.\u0027 This data is used to \u0027train a *new* “reward model”\u0027 which is then used with Reinforcement Learning \u0027to make the LLM “better” (more helpful, less toxic, etc.).\u0027"
    },
    {
        "question":  "How does the text define LLM \u0027Hallucinations\u0027?",
        "options":  [
                        "A deliberate attempt by the LLM to deceive the user",
                        "The LLM admitting that it does not know an answer",
                        "When the LLM confidently produces factually incorrect information or makes up sources",
                        "A type of security attack to trick the model"
                    ],
        "correct_answer":  "When the LLM confidently produces factually incorrect information or makes up sources",
        "motivation":  "The text defines \u0027Hallucinations\u0027 as when an LLM \u0027will confidently... make up facts\u0027 or cite \u0027a non-existent book, paper or documentation bit.\u0027 It emphasizes that this is not \u0027lying\u0027 because the LLM has no intent, it\u0027s just a \u0027plausibility engine\u0027 generating a statistically likely (but factually incorrect) sequence."
    },
    {
        "question":  "Why does the text call an LLM a \u0027plausibility engine,\u0027 not a \u0027truth engine\u0027?",
        "options":  [
                        "Because its primary goal is to retrieve factual information from a database",
                        "Because it is trained to predict the next plausible token in a sequence, not to verify the factual truth of its statements",
                        "Because it has a \u0027knowledge cutoff\u0027 and cannot access new information",
                        "Because it is designed to be a creative writing partner"
                    ],
        "correct_answer":  "Because it is trained to predict the next plausible token in a sequence, not to verify the factual truth of its statements",
        "motivation":  "The text explicitly states: \u0027Recall that its pre-trained model goal is to predict the next plausible token in a sequence. **That’s it**. It is a “plausibility engine”, not a “truth engine”.\u0027 This explains *why* hallucinations happen."
    },
    {
        "question":  "What is a \u0027Prompt Injection\u0027 attack?",
        "options":  [
                        "When an LLM hallucinates and provides incorrect code",
                        "When an LLM\u0027s knowledge is outdated due to its training cutoff",
                        "When a malicious user crafts input to trick the model into ignoring the developer\u0027s instructions and following new, hidden ones",
                        "The process of fine-tuning an LLM for a specific task"
                    ],
        "correct_answer":  "When a malicious user crafts input to trick the model into ignoring the developer\u0027s instructions and following new, hidden ones",
        "motivation":  "The text defines \u0027Prompt injection\u0027 as \u0027an attack where a malicious user crafts their input to trick the model into ignoring the developer\u0027s original instructions and following the user\u0027s new, hidden instructions instead.\u0027"
    },
    {
        "question":  "According to the text, why are LLMs vulnerable to prompt injections?",
        "options":  [
                        "Because they have a knowledge cutoff",
                        "Because they are \u0027plausibility engines\u0027",
                        "Because the LLM cannot fundamentally distinguish between instructions (from a developer) and data (from the user)",
                        "Because they are not trained with RLHF"
                    ],
        "correct_answer":  "Because the LLM cannot fundamentally distinguish between instructions (from a developer) and data (from the user)",
        "motivation":  "The text explains this vulnerability: \u0027...the LLM cannot fundamentally distinguish between instructions (from a developer) and data (from the user). In the end, it\u0027s all just tokens.\u0027 This is the core reason the attack works."
    },
    {
        "question":  "What is the \u0027knowledge cutoff\u0027 of an LLM?",
        "options":  [
                        "The LLM\u0027s inability to understand complex topics",
                        "A security measure to prevent prompt injections",
                        "The fact that an LLM\u0027s knowledge is static and based on its pre-training data, which is from a specific point in the past",
                        "The LLM\u0027s tendency to hallucinate facts"
                    ],
        "correct_answer":  "The fact that an LLM\u0027s knowledge is static and based on its pre-training data, which is from a specific point in the past",
        "motivation":  "The text defines the \u0027knowledge cutoff\u0027 as the state that \u0027its knowledge is not live.\u0027 It is a \u0027static snapshot of the pre-training data\u0027 from \u0027some point in the past,\u0027 and \u0027the model won’t learn anything since.\u0027"
    },
    {
        "question":  "What is \u0027automation bias\u0027 in the context of over-relying on LLMs?",
        "options":  [
                        "The LLM\u0027s bias towards certain topics based on its training data",
                        "The assumption that because the model is articulate and confident, it must be right",
                        "A type of prompt injection attack",
                        "The process of using an LLM to automate software development"
                    ],
        "correct_answer":  "The assumption that because the model is articulate and confident, it must be right",
        "motivation":  "The text describes \u0027automation bias\u0027 as a consequence of over-reliance: \u0027Because the model is so articulate and confident, we develop an “automation bias”: we just assume it’s right.\u0027 This is part of the \u0027ChatGPT told me so\u0027 problem."
    },
    {
        "question":  "What is the text\u0027s correction to the myth \u0027An LLM is just a fancier autocomplete\u0027?",
        "options":  [
                        "This is correct; it is just a larger autocomplete.",
                        "An LLM\u0027s scale unlocks \u0027emergent abilities\u0027—complex behaviors not explicitly programmed, like step-by-step reasoning or writing code.",
                        "An LLM is a \u0027truth engine,\u0027 not an autocomplete tool.",
                        "An LLM is an information retrieval system, which is different from autocomplete."
                    ],
        "correct_answer":  "An LLM\u0027s scale unlocks \u0027emergent abilities\u0027—complex behaviors not explicitly programmed, like step-by-step reasoning or writing code.",
        "motivation":  "The text refutes this myth by stating it\u0027s an \u0027over-simplification.\u0027 It explains that \u0027emergent abilities\u0027 like \u0027step-by-step reasoning (chain-of-thought), writing functional code, and even passing standardized exams\u0027 appear \u0027once the models become large enough.\u0027 This makes them \u0027far more powerful and flexible than simple autocomplete.\u0027"
    },
    {
        "question":  "How does the text differentiate a Search Engine from an LLM?",
        "options":  [
                        "They are the same; an LLM is just a better search engine.",
                        "A Search Engine is a \u0027generator tool\u0027 and an LLM is a \u0027search tool\u0027.",
                        "A Search Engine is an \u0027Information Retrieval system\u0027 (finds *existing* info), while an LLM is an \u0027Information Generation system\u0027 (produces *new* tokens).",
                        "A Search Engine is vulnerable to hallucinations, while an LLM is not."
                    ],
        "correct_answer":  "A Search Engine is an \u0027Information Retrieval system\u0027 (finds *existing* info), while an LLM is an \u0027Information Generation system\u0027 (produces *new* tokens).",
        "motivation":  "The text makes this distinction very clear: \u0027A Search Engine... is an “Information Retrieval system”... finding **existing** information... An LLM is an Information Generation system. It produces **new** tokens out of previously seen ones.\u0027"
    },
    {
        "question":  "According to the text, what is the *wrong* way to use an LLM that leads to hallucinations?",
        "options":  [
                        "Asking it to brainstorm creative ideas",
                        "Asking it to rewrite an email in a professional tone",
                        "Asking it to summarize an article",
                        "Using it to retrieve facts, asking a \u0027generator tool\u0027 to do the job of a \u0027search tool\u0027"
                    ],
        "correct_answer":  "Using it to retrieve facts, asking a \u0027generator tool\u0027 to do the job of a \u0027search tool\u0027",
        "motivation":  "The text explicitly concludes its comparison of search engines and LLMs with this warning: \u0027**Using an LLM to retrieve facts is exactly what leads to hallucinations.** **You are asking a “generator tool” to do the job of a “search tool”.**\u0027"
    },
    {
        "question":  "What does the \u0027C\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Code",
                        "Creativity",
                        "Context",
                        "Clarity"
                    ],
        "correct_answer":  "Context",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027C\u0027 stands for Context, defined as the \u0027background information we need to understand the “universe” of any statement.\u0027"
    },
    {
        "question":  "What does the \u0027R\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Role",
                        "Reasoning",
                        "Retrieval",
                        "Response"
                    ],
        "correct_answer":  "Role",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027R\u0027 stands for Role, which \u0027heavily guides the model\u0027s tone and style.\u0027"
    },
    {
        "question":  "What does the \u0027O\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Output",
                        "Objective",
                        "Optimization",
                        "Options"
                    ],
        "correct_answer":  "Objective",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027O\u0027 stands for Objective, which means \u0027state the deliverable you expect, clearly and directly.\u0027"
    },
    {
        "question":  "What does the \u0027P\u0027 stand for in the C.R.O.P. prompting framework?",
        "options":  [
                        "Prompt",
                        "Product",
                        "Plausibility",
                        "Parameters"
                    ],
        "correct_answer":  "Parameters",
        "motivation":  "The text introduces the C.R.O.P. Framework as: **C**ontext, **R**ole, **O**bjective, **P**arameters. The \u0027P\u0027 stands for Parameters, which are \u0027additional **constraints**\u0027 that \u0027help deliver results more close to our request,\u0027 especially for coding."
    },
    {
        "question":  "In the \u0027bad prompt\u0027 example for the URL shortener, what was a key failure?",
        "options":  [
                        "The prompt was too short and did not provide enough detail",
                        "The prompt asked for a technical design plan instead of code",
                        "The prompt was a \u0027stream of consciousness\u0027 that lacked clear constraints and context (e.g., internal vs. public tool)",
                        "The prompt specified the wrong tech stack (Elixir and Phoenix)"
                    ],
        "correct_answer":  "The prompt was a \u0027stream of consciousness\u0027 that lacked clear constraints and context (e.g., internal vs. public tool)",
        "motivation":  "The text analyzes the \u0027bad prompt\u0027 as a \u0027long “stream of consciousness” prompt that *feels* detailed - but in reality is actually unfocused, lacks clear constraints... There’s no clear context: we’re not mentioning this shortener is an internal-use tool.\u0027"
    },
    {
        "question":  "In the \u0027good prompt\u0027 example for the URL shortener, what was a key *deliverable* requested from the LLM?",
        "options":  [
                        "All the Elixir and Phoenix code for the controller and Ecto schema",
                        "A comprehensive technical design plan, including routes, schema, and logic, but *no* Elixir or Phoenix code",
                        "A long \u0027stream of consciousness\u0027 brain-dump about URL shorteners",
                        "A list of alternative tech stacks to use instead of Elixir"
                    ],
        "correct_answer":  "A comprehensive technical design plan, including routes, schema, and logic, but *no* Elixir or Phoenix code",
        "motivation":  "The \u0027good prompt\u0027 explicitly instructs the LLM to \u0027Provide a comprehensive technical design plan\u0027 and, crucially, \u0027Do not write any Elixir or Phoenix code.\u0027 The goal was to define the system\u0027s components *before* implementation."
    },
    {
        "question":  "In the proposed software development framework, what is the \u0027Human Phase\u0027?",
        "options":  [
                        "The phase where the human blindly copies and pastes code from the LLM",
                        "The phase where the human provides the initial creative spark and the LLM does all the thinking and planning",
                        "The phase where the human *must think* and provide sketches, tech constraints, and a product description *before* asking the LLM to act",
                        "The phase where the human ranks the LLM\u0027s outputs for RLHF"
                    ],
        "correct_answer":  "The phase where the human *must think* and provide sketches, tech constraints, and a product description *before* asking the LLM to act",
        "motivation":  "The text emphasizes that to *properly* use LLMs, \u0027we must think, and quite a lot.\u0027 The \u0027Human Phase\u0027 involves the human expert doing upfront work: \u0027Research\u0027 (providing sketches/screenshots), defining \u0027Tech constraints,\u0027 and writing a \u0027Product (quick!) description.\u0027"
    },
    {
        "question":  "In the \u0027Agentic Phase\u0027 of software development, what is the \u0027Product Owner Agent\u0027 tasked with doing?",
        "options":  [
                        "Writing the final Elixir and Phoenix code",
                        "Creating a detailed multi-step implementation plan for the IDE Agent",
                        "Taking the human\u0027s sketches and requirements and expanding them into a full \u0027PRD document\u0027 (Product Requirements Document)",
                        "Finding bugs in the human\u0027s \u0027tech constraints\u0027 document"
                    ],
        "correct_answer":  "Taking the human\u0027s sketches and requirements and expanding them into a full \u0027PRD document\u0027 (Product Requirements Document)",
        "motivation":  "The text describes the \u0027Product Owner Agent\u0027 step as feeding the human-generated documents to a \u0027deep think model\u0027 and asking for a \u0027“product owner”-like document.\u0027 The output is a \u0027lengthy “PRD document” which explains our product, fully.\u0027"
    },
    {
        "question":  "In the \u0027Agentic Phase\u0027, why is it important for the \u0027Software Development Agent\u0027 to create a *plan* before writing code?",
        "options":  [
                        "This step is not important and can be skipped",
                        "To give the human something to read while the LLM writes the code",
                        "Because the agent needs to \u0027connect the dots\u0027 and understand *how* to apply the \u0027what\u0027 (the PRD) to the existing codebase",
                        "To generate a .txt file, which is the only format the LLM can write to"
                    ],
        "correct_answer":  "Because the agent needs to \u0027connect the dots\u0027 and understand *how* to apply the \u0027what\u0027 (the PRD) to the existing codebase",
        "motivation":  "The text stresses the importance of this planning step: \u0027We’ve fed the “what”, and this planning step will formulate a “how”. This enables the Agent to read the whole codebase, and now it’s able to “connect the dots”. This can’t be done (properly) in just one step!\u0027"
    },
    {
        "question":  "What is the final action the human is told to take after the \u0027Software Development Agent\u0027 creates its implementation plan?",
        "options":  [
                        "Immediately run the agent and let it code",
                        "Delete the plan and write the code yourself",
                        "**Read it.** And then edit or ask for changes *before* the agent implements them",
                        "Thank the agent and close the program"
                    ],
        "correct_answer":  "**Read it.** And then edit or ask for changes *before* the agent implements them",
        "motivation":  "The text is very explicit about this: \u0027Finally, we’ve asked your agent to save the output of the plan into an editable text file. **Read it.** Again, **don’t skip this step.** We can (finally) edit or ask for some changes *before* the agent implements those.\u0027"
    },
    {
        "question":  "What core concept from calculus is used to find the \u0027direction of steepest ascent\u0027 in a multi-dimensional function?",
        "options":  [
                        "Dot Product",
                        "Matrix",
                        "Derivative",
                        "Gradient (∇f)"
                    ],
        "correct_answer":  "Gradient (∇f)",
        "motivation":  "The text defines the derivative `f\u0027(x)` as the rate of change for a simple function. It then introduces the gradient `∇f` as \u0027the multi-dimensional version of a derivative\u0027 and states it \u0027is a vector that **points in the direction of the steepest ascent**.\u0027"
    },
    {
        "question":  "Why does the Stochastic Gradient Descent algorithm take a step in the *opposite* direction of the gradient?",
        "options":  [
                        "To maximize the error function",
                        "To find the direction of steepest ascent (go uphill)",
                        "To minimize the error (loss function) by going \u0027downhill\u0027 in the \u0027hilly landscape\u0027 of errors",
                        "To move to a random point in the landscape"
                    ],
        "correct_answer":  "To minimize the error (loss function) by going \u0027downhill\u0027 in the \u0027hilly landscape\u0027 of errors",
        "motivation":  "The text explains the goal is to \u0027minimize errors\u0027 by finding the \u0027lowest valley\u0027 in the \u0027hilly landscape\u0027 (the loss function). Since the gradient \u0027points uphill,\u0027 the algorithm takes \u0027a small step in **the opposite direction** (aka: go downhill)\u0027 to move towards the minimum."
    },
    {
        "question":  "What is the text\u0027s definition of a \u0027deterministic\u0027 system?",
        "options":  [
                        "A system with inherent randomness",
                        "A system where the same input can lead to different outputs",
                        "A system where the output is perfectly predictable from the input and involves no randomness",
                        "A machine learning model that is \u0027Probably Approximately Correct\u0027"
                    ],
        "correct_answer":  "A system where the output is perfectly predictable from the input and involves no randomness",
        "motivation":  "The text defines deterministic systems as: \u0027when the output *is perfectly predictable* from the input. No randomness is involved. The same input leads to the same output.\u0027"
    },
    {
        "question":  "What is the formal definition of the \u0027training data\u0027 in machine learning?",
        "options":  [
                        "The prediction rule `h: X→Y`",
                        "The set of all possible objects, `X`",
                        "A set of pairs of \u0027objects / labels\u0027, mathematically `S = {(x1,y1), ..., (xm, ym}`",
                        "The \u0027ground truth\u0027 function, `f`"
                    ],
        "correct_answer":  "A set of pairs of \u0027objects / labels\u0027, mathematically `S = {(x1,y1), ..., (xm, ym}`",
        "motivation":  "The text\u0027s \u0027complete definition\u0027 of learning lists \u0027A set of “training data”... mathematically: **`S = {(x1,y1), ..., (xm, ym}`**\u0027 which contains \u0027pairs of “objects / labels”\u0027."
    },
    {
        "question":  "What is the \u0027Mean Squared Error (MSE)\u0027 used for in the car dealership example?",
        "options":  [
                        "To measure the empirical error (training error) of the model",
                        "To set the initial random values for `weight` and `bias`",
                        "To calculate the gradient of the loss function",
                        "To define the \u0027small step\u0027 size in gradient descent"
                    ],
        "correct_answer":  "To measure the empirical error (training error) of the model",
        "motivation":  "The text states, \u0027To answer this question [How good is our computed model?], we need to measure its error on the data it was trained on. **That is the empirical error (or training error)**.\u0027 It then introduces Mean Squared Error (MSE) as \u0027A common way to measure it.\u0027"
    },
    {
        "question":  "Why was the 4th-degree polynomial model in the car example \u0027perfect\u0027 on the training data but bad in practice?",
        "options":  [
                        "It was too simple and \u0027underfit\u0027 the data",
                        "It was a Linear Regression model in disguise",
                        "It had \u0027overfit\u0027 the data, memorizing its random noise instead of the general trend, leading to poor generalization",
                        "It failed to achieve a training error of zero"
                    ],
        "correct_answer":  "It had \u0027overfit\u0027 the data, memorizing its random noise instead of the general trend, leading to poor generalization",
        "motivation":  "The text explains this as the \u0027overfitting trap.\u0027 The model \u0027memorized the training data, and adapted way too much, aka **including its random noise**, instead of “learning the general trend”.\u0027 This caused it to make unreasonable predictions (like a negative price) on new data."
    },
    {
        "question":  "What is the key takeaway from the \u0027overfitting trap\u0027 example?",
        "options":  [
                        "Always use the most complex model possible to get the training error to zero",
                        "A low training error is the primary and most important goal of machine learning",
                        "Linear models are always better than polynomial models",
                        "A low training error does not guarantee good performance on new data; the goal is to generalize, not memorize"
                    ],
        "correct_answer":  "A low training error does not guarantee good performance on new data; the goal is to generalize, not memorize",
        "motivation":  "The text concludes this section by stating, \u0027This demonstrates that having a low training error isn\u0027t our primary goal.\u0027 The real goal is to \u0027find a model `h` so that it can perform nicely in real world scenarios,\u0027 which is the definition of generalization."
    },
    {
        "question":  "In the formal definition of Reinforcement Learning, what is the \u0027State (S)\u0027?",
        "options":  [
                        "The agent\u0027s strategy or function",
                        "The reward the agent receives",
                        "A description of the environment at a specific moment",
                        "The action the agent chooses"
                    ],
        "correct_answer":  "A description of the environment at a specific moment",
        "motivation":  "The text\u0027s formal definitions for RL list \u0027State (S): A snapshot of the environment. E.g., the position of all pieces on a chessboard.\u0027"
    },
    {
        "question":  "What is an example of \u0027Incorrect or Inaccurate Data\u0027 provided in the text?",
        "options":  [
                        "A hiring dataset that is 90% male",
                        "Customer information where half the age entries are blank",
                        "A sensor recording a temperature of -200°C in Udine in July",
                        "A dataset that is split into training, validation, and test sets"
                    ],
        "correct_answer":  "A sensor recording a temperature of -200°C in Udine in July",
        "motivation":  "The text provides this as a specific example of \u0027Incorrect or Inaccurate Data\u0027: \u0027a sensor records a temperature of -200°C in Udine, in July.\u0027 The hiring data is \u0027Biased Data\u0027 and the blank age entries are \u0027Missing Data.\u0027"
    },
    {
        "question":  "What is the purpose of the \u0027Model Training\u0027 step in the core ML workflow?",
        "options":  [
                        "To clean, format, and transform the data",
                        "To gather raw data from various sources",
                        "To use the \u0027training set\u0027 to teach the model to find patterns and adjust its parameters",
                        "To deploy the model into a live production environment"
                    ],
        "correct_answer":  "To use the \u0027training set\u0027 to teach the model to find patterns and adjust its parameters",
        "motivation":  "The text describes the \u0027Model Training\u0027 step as the one where \u0027the model “learns”.\u0027 It specifies that \u0027The “training set” is used to actually learn the patterns and adjust the model’s parameters (like the `weight` and `bias`...)\u0027"
    },
    {
        "question":  "In the core ML workflow, what is the \u0027Deployment\u0027 step?",
        "options":  [
                        "The process of cleaning and formatting data",
                        "The final, one-time evaluation of the model on the test set",
                        "The process of integrating the trained model into a real-world application or system",
                        "The process of splitting data into K-Folds for cross-validation"
                    ],
        "correct_answer":  "The process of integrating the trained model into a real-world application or system",
        "motivation":  "The text lists \u0027Deployment\u0027 as step 4 of the core workflow, describing it as \u0027Putting the model into a production environment (e.g., inside an app, on a server) where it can make predictions on new, real-world data.\u0027"
    },
    {
        "question":  "What historical phase of LLMs is associated with RNNs (Recurrent Neural Networks) and LSTMs?",
        "options":  [
                        "Phase 1: Symbolic AI",
                        "Phase 2: Statistical",
                        "Phase 3: RNNs/LSTMs (Sequential)",
                        "Phase 4: The Transformer"
                    ],
        "correct_answer":  "Phase 3: RNNs/LSTMs (Sequential)",
        "motivation":  "The text\u0027s history of LLMs lists \u0027Phase 3: RNNs/LSTMs (Sequential)\u0027 and describes them as \u0027the first model types that could handle sequential data (like text) by using... a “memory” (or *state*).\u0027"
    },
    {
        "question":  "What was the main limitation of RNNs that the Transformer architecture aimed to solve?",
        "options":  [
                        "RNNs could not process text sequentially",
                        "RNNs\u0027 sequential, one-word-at-a-time processing and \u0027singleton memory\u0027 was a bottleneck",
                        "RNNs were \u0027plausibility engines,\u0027 not \u0027truth engines\u0027",
                        "RNNs were vulnerable to prompt injections"
                    ],
        "correct_answer":  "RNNs\u0027 sequential, one-word-at-a-time processing and \u0027singleton memory\u0027 was a bottleneck",
        "motivation":  "The text explains that the 2017 Transformer paper \u0027abandoned the previously adopted sequential approach of RNNs.\u0027 The Transformer\u0027s idea was \u0027instead of keeping a “singleton memory”, composed by reading input text one-word-at-a-time, to process and find the entire sentence context at once.\u0027"
    },
    {
        "question":  "What is \u0027Fine-Tuning\u0027 in the LLM architecture?",
        "options":  [
                        "The initial unsupervised training on web-scale data",
                        "The process of breaking text into numerical tokens",
                        "A *supervised* training step on a smaller, specific dataset to teach the model a particular task (e.g., summarization)",
                        "The process of using human feedback to make the model \u0027safer\u0027"
                    ],
        "correct_answer":  "A *supervised* training step on a smaller, specific dataset to teach the model a particular task (e.g., summarization)",
        "motivation":  "The text describes \u0027Fine-Tuning\u0027 as a step after pre-training. It is a \u0027supervised, training step\u0027 where the model is \u0027trained on a much smaller, *curated* dataset... for a *specific* task\u0027 (e.g., \u0027a dataset of questions and answers\u0027)."
    },
    {
        "question":  "According to the text, what is the myth about LLMs \u0027understanding\u0027 human language?",
        "options":  [
                        "LLMs truly understand the meaning and intent behind words, just like humans",
                        "LLMs \u0027understanding\u0027 is just a \u0027statistical map of which tokens are likely to follow other tokens\u0027 in a given context",
                        "LLMs\u0027 understanding comes from the \u0027Fine-Tuning\u0027 step, not the \u0027Pre-training\u0027 step",
                        "LLMs can only understand text, not code"
                    ],
        "correct_answer":  "LLMs \u0027understanding\u0027 is just a \u0027statistical map of which tokens are likely to follow other tokens\u0027 in a given context",
        "motivation":  "The text refutes the myth \u0027Can an LLM *understand* me?\u0027 by stating \u0027No.\u0027 It explains: \u0027It’s all about statistics... What we perceive as “understanding” is just an incredibly complex and massive statistical map of which tokens are likely to follow other tokens...\u0027"
    },
    {
        "question":  "What is the key difference between \u0027information retrieval\u0027 and \u0027information generation\u0027?",
        "options":  [
                        "Retrieval finds *existing* information, while generation produces *new* information (tokens)",
                        "Retrieval is what LLMs do, and generation is what search engines do",
                        "Retrieval is always factual, while generation is always fictional",
                        "There is no difference; they are two terms for the same process"
                    ],
        "correct_answer":  "Retrieval finds *existing* information, while generation produces *new* information (tokens)",
        "motivation":  "The text introduces these terms to contrast Search Engines and LLMs. A Search Engine (retrieval) is for \u0027finding **existing** information.\u0027 An LLM (generation) \u0027produces **new** tokens out of previously seen ones.\u0027"
    },
    {
        "question":  "In the C.R.O.P. framework, what is the purpose of the \u0027Role\u0027 (R) parameter?",
        "options":  [
                        "To provide the background information and context of the request",
                        "To state the clear, unambiguous deliverable that is expected",
                        "To add constraints, such as \u0027output as JSON\u0027",
                        "To guide the model\u0027s tone, style, and the knowledge-base it draws from (e.g., \u0027Act as a computer science professor\u0027)"
                    ],
        "correct_answer":  "To guide the model\u0027s tone, style, and the knowledge-base it draws from (e.g., \u0027Act as a computer science professor\u0027)",
        "motivation":  "The text explains the \u0027Role\u0027 parameter \u0027heavily guides the model\u0027s tone and style of the response.\u0027 It also suggests telling the LLM who *you* are to \u0027influence the “knowledge-base” it draws the attention from.\u0027"
    },
    {
        "question":  "What is a \u0027PRD document\u0027 as described in the \u0027Agentic Phase\u0027 of software development?",
        "options":  [
                        "A file containing the final, production-ready code",
                        "A document defining the \u0027core software engineering principles\u0027",
                        "A \u0027Product Requirements Document\u0027 that an LLM (acting as a Product Owner) expands from the human\u0027s initial sketches and notes",
                        "A text file containing the multi-step implementation plan from the \u0027Software Development Agent\u0027"
                    ],
        "correct_answer":  "A \u0027Product Requirements Document\u0027 that an LLM (acting as a Product Owner) expands from the human\u0027s initial sketches and notes",
        "motivation":  "The text describes the \u0027Product Owner Agent\u0027 step\u0027s goal as creating a \u0027PRD document.\u0027 This is a \u0027lengthy “PRD document” which explains our product, fully,\u0027 generated *from* the human\u0027s inputs (sketches, constraints, description)."
    },
    {
        "question":  "What does the text mean by \u0027anthropomorphize\u0027 in the context of LLMs?",
        "options":  [
                        "To treat LLMs as simple autocomplete tools",
                        "To use LLMs for information retrieval instead of generation",
                        "To attribute human-like qualities such as \u0027thinks,\u0027 \u0027knows,\u0027 or \u0027understands\u0027 to LLMs",
                        "To use the C.R.O.P. framework for prompting"
                    ],
        "correct_answer":  "To attribute human-like qualities such as \u0027thinks,\u0027 \u0027knows,\u0027 or \u0027understands\u0027 to LLMs",
        "motivation":  "The text states, \u0027We, humans, are quite the egocentric beings. We tend to *anthropomorphize* everything. LLMs are no exception: we use wordings like “it thinks”, “it says”, “it lied” or “it understands me”.\u0027"
    },
    {
        "question":  "In the \u0027Human Phase\u0027 of software development with LLMs, what is the \u0027Tech constraints\u0027 output?",
        "options":  [
                        "A full architecture document for the application, written by the human",
                        "A small \u0027structure document\u0027 explaining the human\u0027s \u0027core software engineering principles\u0027 (e.g., frameworks, libraries)",
                        "Sketches and screenshots of the desired result",
                        "A lengthy \u0027PRD document\u0027 written by the human"
                    ],
        "correct_answer":  "A small \u0027structure document\u0027 explaining the human\u0027s \u0027core software engineering principles\u0027 (e.g., frameworks, libraries)",
        "motivation":  "The text defines the \u0027Tech constraints\u0027 step as the human putting on the \u0027architect hat\u0027 to define \u0027languages, frameworks, libraries, core software principles.\u0027 The output is \u0027a small “structure document” explaining our “core software engineering principles”.\u0027"
    },
    {
        "question":  "What is a dot product, as described in the text?",
        "options":  [
                        "A grid of numbers representing a dataset",
                        "The multi-dimensional version of a derivative",
                        "An operation that tells how aligned two vectors are, calculated by multiplying corresponding elements and summing the results",
                        "A cluster of common information describing a single data point"
                    ],
        "correct_answer":  "An operation that tells how aligned two vectors are, calculated by multiplying corresponding elements and summing the results",
        "motivation":  "The text defines a dot product as \u0027a fundamental operation that tells us *how aligned two vectors are*.\u0027 It provides an example, \u0027A⋅B = 1*3 + 2*4 = 11\u0027, which shows the process of multiplying corresponding elements and summing them."
    },
    {
        "question":  "What does a dot product value near zero imply?",
        "options":  [
                        "The two vectors point in a similar direction",
                        "The two vectors point in opposite directions",
                        "The two vectors are unrelated (geometrically perpendicular)",
                        "The two vectors are identical"
                    ],
        "correct_answer":  "The two vectors are unrelated (geometrically perpendicular)",
        "motivation":  "The text states: \u0027a value near zero means two vectors are geometrically perpendicular, aka **they are unrelated**.\u0027"
    },
    {
        "question":  "What is the text\u0027s view on the Elixir programming language?",
        "options":  [
                        "It is a complex language that is not suitable for machine learning",
                        "It is a language that relies heavily on \u0027mutability\u0027 and classical \u0027loops\u0027",
                        "It is a functional programming language that has no \u0027mutability\u0027 and uses recursion, which will be used for ML exercises",
                        "It is a language primarily used for statistical analysis and calculus"
                    ],
        "correct_answer":  "It is a functional programming language that has no \u0027mutability\u0027 and uses recursion, which will be used for ML exercises",
        "motivation":  "The text introduces Elixir as a \u0027functional programming language\u0027 that will \u0027enable us to do some hands-on machine learning exercises!\u0027 It specifically notes that \u0027Elixir has no *mutability*\u0027 and \u0027you won’t find “loops” in a classical sense: only recursion is accepted.\u0027"
    },
    {
        "question":  "What is the formal definition of \u0027ML\u0027 provided in the text?",
        "options":  [
                        "The science of getting computers to act without being *explicitly* programmed",
                        "A method for finding a \u0027ground truth\u0027 function `f` that is perfectly accurate",
                        "A system for writing \u0027dumb by-hand rules\u0027 to classify data",
                        "The process of using a search engine to retrieve information"
                    ],
        "correct_answer":  "The science of getting computers to act without being *explicitly* programmed",
        "motivation":  "The text in \u0027Machine Learning 101\u0027 provides this exact definition: \u0027Machine Learning (ML) is the science of getting computers to act without being *explicitly* programmed.\u0027"
    },
    {
        "question":  "In the \u0027ML in a nutshell\u0027 recipe, what is the third step?",
        "options":  [
                        "Fetch some data, call it \u0027training set\u0027",
                        "Define a model, define its parameters, and run it on the training set",
                        "Tune the parameters, so that it minimizes the empirical error on the training set",
                        "Deploy the model to a production environment"
                    ],
        "correct_answer":  "Tune the parameters, so that it minimizes the empirical error on the training set",
        "motivation":  "The text summarizes \u0027Machine Learning, in a nutshell\u0027 in three steps. The third step is: \u0027Tune the parameters, so that it minimizes the empirical error on the training set.\u0027"
    },
    {
        "question":  "In the car dealership example, what is the \u0027bias\u0027 in the linear model `predicted_price = weight * mileage + bias`?",
        "options":  [
                        "The parameter that controls the relationship between mileage and price",
                        "The empirical error of the model",
                        "The starting price of a car, or the predicted price when the mileage is zero",
                        "The \u0027small step\u0027 taken during gradient descent"
                    ],
        "correct_answer":  "The starting price of a car, or the predicted price when the mileage is zero",
        "motivation":  "In a linear model `y = mx + b`, the \u0027b\u0027 term (here, \u0027bias\u0027) is the y-intercept, which is the value of `y` when `x` is 0. In this context, it represents the predicted price (`y`) when the mileage (`x`) is 0."
    },
    {
        "question":  "In the car dealership example, what is the \u0027small step\u0027 (or learning rate) used for?",
        "options":  [
                        "It is the final `weight` parameter of the trained model",
                        "It is the `bias` parameter of the trained model",
                        "It controls how much the `weight` and `bias` are adjusted during each iteration of gradient descent",
                        "It is the Mean Squared Error of the model"
                    ],
        "correct_answer":  "It controls how much the `weight` and `bias` are adjusted during each iteration of gradient descent",
        "motivation":  "The text describes \u0027Step 3\u0027 of the gradient descent example as choosing a \u0027“small step” to be `0.001`.\u0027 It\u0027s crucial because \u0027Too big, and we leap too far. Too small, and we won’t ever reach the minimum error.\u0027 This step value is used in \u0027Step 4\u0027 to update the parameters."
    },
    {
        "question":  "What is the key difference between Supervised and Unsupervised Learning?",
        "options":  [
                        "Supervised learning uses a \u0027reward\u0027 system, while Unsupervised learning does not",
                        "Supervised learning data is \u0027labeled\u0027 (has inputs and correct outputs), while Unsupervised learning data is \u0027unlabeled\u0027",
                        "Supervised learning finds hidden patterns, while Unsupervised learning learns a mapping function",
                        "Supervised learning is used for LLMs, while Unsupervised learning is used for linear regression"
                    ],
        "correct_answer":  "Supervised learning data is \u0027labeled\u0027 (has inputs and correct outputs), while Unsupervised learning data is \u0027unlabeled\u0027",
        "motivation":  "The text draws a clear line: \u0027Supervised Learning: When the data is labeled. Goal: To learn a mapping function...\u0027. In contrast: \u0027Unsupervised Learning: The data is unlabeled. Goal: To find hidden structures and patterns...\u0027"
    },
    {
        "question":  "In the Reinforcement Learning analogy, who is the \u0027Agent\u0027?",
        "options":  [
                        "The human player",
                        "The set of rules for the game",
                        "The \u0027world\u0027 or \u0027game board\u0027 itself",
                        "The reward or \u0027Good boy!\u0027"
                    ],
        "correct_answer":  "The human player",
        "motivation":  "The text provides an analogy for RL: \u0027You (Agent) are playing a video game (Environment). You press a button (Action) while at a certain screen (State). The game gives you points (Reward)...\u0027"
    },
    {
        "question":  "In the core ML workflow, what is the \u0027Monitoring \u0026 Maintenance\u0027 step?",
        "options":  [
                        "The initial gathering of raw data",
                        "The process of cleaning and formatting data",
                        "The one-time deployment of the model",
                        "The ongoing process of tracking the model\u0027s performance in production and retraining it as needed"
                    ],
        "correct_answer":  "The ongoing process of tracking the model\u0027s performance in production and retraining it as needed",
        "motivation":  "The text lists \u0027Monitoring \u0026 Maintenance\u0027 as the final step in the workflow: \u0027Tracking the model’s performance in the real world. Over time, data can change... requiring the model to be retrained or updated.\u0027"
    },
    {
        "question":  "What is the \u0027Transformer\u0027 in the context of LLMs?",
        "options":  [
                        "An early form of Symbolic AI from the 1960s",
                        "A type of RNN that uses a \u0027singleton memory\u0027",
                        "The 2017 architecture introduced in \u0027Attention Is All You Need\u0027 that processes sentence context all at once using self-attention",
                        "The process of cleaning and transforming data before training"
                    ],
        "correct_answer":  "The 2017 architecture introduced in \u0027Attention Is All You Need\u0027 that processes sentence context all at once using self-attention",
        "motivation":  "The text identifies the \u0027Transformer\u0027 as the \u0027new architecture\u0027 from the 2017 paper \u0027Attention Is All You Need.\u0027 Its core idea is \u0027to process and find the entire sentence context at once\u0027 using \u0027self-attention.\u0027"
    },
    {
        "question":  "What is the \u0027plausibility engine\u0027 vs \u0027truth engine\u0027 distinction meant to explain?",
        "options":  [
                        "Why LLMs are so accurate at retrieving facts",
                        "Why LLMs \u0027hallucinate\u0027 (they predict the next plausible token, not the next *true* token)",
                        "Why LLMs are vulnerable to prompt injections",
                        "Why LLMs have a knowledge cutoff"
                    ],
        "correct_answer":  "Why LLMs \u0027hallucinate\u0027 (they predict the next plausible token, not the next *true* token)",
        "motivation":  "The text uses this distinction to explain hallucinations: \u0027The LLM is not “lying”... its pre-trained model goal is to predict the next plausible token in a sequence... It is a “plausibility engine”, not a “truth engine”.\u0027"
    },
    {
        "question":  "What is the text\u0027s advice for a human developer *before* using an LLM in the \u0027Agentic Phase\u0027?",
        "options":  [
                        "To ask the LLM for a plan, then blindly trust it",
                        "To do the \u0027Human Phase\u0027 work first: provide sketches, tech constraints, and a product description",
                        "To write a long \u0027stream of consciousness\u0027 prompt with all your ideas",
                        "To use the LLM as a search engine to find the right code"
                    ],
        "correct_answer":  "To do the \u0027Human Phase\u0027 work first: provide sketches, tech constraints, and a product description",
        "motivation":  "The text introduces the \u0027Agentic Phase\u0027 as something that happens *after* the \u0027Human Phase.\u0027 The \u0027Human Phase\u0027 requires the human to \u0027provide a simple sketch,\u0027 define \u0027Tech constraints,\u0027 and write a \u0027Product (quick!) description\u0027 *before* the agent is asked to do work."
    },
    {
        "question":  "What is a \u0027heuristic\u0027 as defined in the source material?",
        "options":  [
                        "A perfect, analytical solution to a problem",
                        "A rule-of-thumb or an \u0027educated guess\u0027 used to find a good-enough solution when a perfect one is too hard or slow",
                        "The \u0027ground truth\u0027 function `f` in machine learning",
                        "A deterministic algorithm that always gives the same output"
                    ],
        "correct_answer":  "A rule-of-thumb or an \u0027educated guess\u0027 used to find a good-enough solution when a perfect one is too hard or slow",
        "motivation":  "The text defines \u0027Heuristics\u0027 as \u0027a rule-of-thumb, or a “educated guess”... when we are not interested in the *perfect* solution, but we just need a *good-enough* one... because the analytical (perfect) one would be too hard, or too slow.\u0027"
    },
    {
        "question":  "In the \u0027ML in a nutshell\u0027 recipe, what is the first step?",
        "options":  [
                        "Define a model",
                        "Tune the parameters",
                        "Fetch some data, call it \u0027training set\u0027",
                        "Deploy the model"
                    ],
        "correct_answer":  "Fetch some data, call it \u0027training set\u0027",
        "motivation":  "The text provides a three-step \u0027recipe\u0027 for ML. The very first step is: \u0027Fetch some data, call it “training set” (`S`).\u0027"
    },
    {
        "question":  "In the \u0027ML in a nutshell\u0027 recipe, what is the second step, after fetching data?",
        "options":  [
                        "Define a model, define its parameters, and run it on the training set",
                        "Tune the parameters to minimize error",
                        "Split the data into training, validation, and test sets",
                        "Gather the results and re-iterate"
                    ],
        "correct_answer":  "Define a model, define its parameters, and run it on the training set",
        "motivation":  "The text\u0027s three-step \u0027recipe\u0027 for ML lists the second step as: \u0027Define a model (`h`, e.g. a linear regression), define its parameters (`weight`, `bias`), and run it on `S`.\u0027"
    },
    {
        "question":  "In the car dealership example, after \u0027500 iterations\u0027 of gradient descent, what were the optimized parameters?",
        "options":  [
                        "`weight` = 0.5, `bias` = 10000",
                        "`weight` = -0.14, `bias` = 33125",
                        "`weight` = -0.25, `bias` = 20000",
                        "`weight` = 0.0, `bias` = 0.0"
                    ],
        "correct_answer":  "`weight` = -0.14, `bias` = 33125",
        "motivation":  "The text shows the result of the optimization process (Stochastic Gradient Descent) after 500 iterations: \u0027`weight` = -0.14\u0027 and \u0027`bias` = 33125\u0027. This represents the learned model."
    },
    {
        "question":  "What is the \u0027Data Gathering\u0027 step in the core ML workflow?",
        "options":  [
                        "Cleaning and formatting the data",
                        "The first step, involving collecting raw data from sources like databases, APIs, or files",
                        "Splitting the data into training and test sets",
                        "Deploying the model to production"
                    ],
        "correct_answer":  "The first step, involving collecting raw data from sources like databases, APIs, or files",
        "motivation":  "The text lists \u0027Data Gathering\u0027 as the very first step (Step 1) of the \u0027core workflow\u0027 for an ML project. It is described as \u0027Gathering the raw data (from databases, APIs, files, etc.).\u0027"
    },
    {
        "question":  "What was \u0027Phase 1: Symbolic AI\u0027 in the history of LLMs?",
        "options":  [
                        "The phase when the Transformer architecture was invented",
                        "The phase using RNNs and LSTMs for sequential data",
                        "The 1960s-1980s approach based on \u0027dumb by-hand rules\u0027 trying to model human \u0027common sense\u0027",
                        "The phase using statistical models to find word relationships"
                    ],
        "correct_answer":  "The 1960s-1980s approach based on \u0027dumb by-hand rules\u0027 trying to model human \u0027common sense\u0027",
        "motivation":  "The text describes \u0027Phase 1: Symbolic AI (1960s-1980s)\u0027 as the initial attempt \u0027to model the human “common sense”\u0027 using \u0027a *ton* of “dumb by-hand rules”.\u0027"
    },
    {
        "question":  "What was \u0027Phase 2: Statistical\u0027 in the history of LLMs?",
        "options":  [
                        "The 1990s-2000s approach that \u0027ditched the rules\u0027 and used statistics to find word relationships",
                        "The 1960s approach using \u0027dumb by-hand rules\u0027",
                        "The 2017+ phase using the Transformer architecture",
                        "The 2010s phase using RNNs and LSTMs"
                    ],
        "correct_answer":  "The 1990s-2000s approach that \u0027ditched the rules\u0027 and used statistics to find word relationships",
        "motivation":  "The text lists \u0027Phase 2: Statistical (1990s-2000s)\u0027 as the period that \u0027ditched the rules, and it was *all* about statistics: which word is close to which other one? Which word follows which one?\u0027"
    },
    {
        "question":  "In the LLM Architecture, what is the \u0027Output\u0027 step?",
        "options":  [
                        "The \u0027Pre-training\u0027 step on a massive dataset",
                        "The process of breaking text into numerical tokens",
                        "The final step where the model generates the next token (word) based on all previous calculations",
                        "The \u0027Embedding\u0027 step where tokens are converted to \u0027Meaning Vectors\u0027"
                    ],
        "correct_answer":  "The final step where the model generates the next token (word) based on all previous calculations",
        "motivation":  "The text\u0027s diagram of the LLM Architecture lists \u0027Output\u0027 as the final step (Step 6), which is the \u0027generation of the next token\u0027 after the pre-training, fine-tuning, and RLHF steps have been completed."
    },
    {
        "question":  "What is the \u0027ChatGPT told me so\u0027 problem described in the text?",
        "options":  [
                        "The fact that ChatGPT is always correct",
                        "A type of prompt injection attack",
                        "The problem of \u0027automation bias,\u0027 where users blindly trust an LLM\u0027s confident-sounding but incorrect output",
                        "The \u0027knowledge cutoff\u0027 that prevents LLMs from accessing new information"
                    ],
        "correct_answer":  "The problem of \u0027automation bias,\u0027 where users blindly trust an LLM\u0027s confident-sounding but incorrect output",
        "motivation":  "The text describes this problem under \u0027Over-reliance\u0027 and \u0027automation bias.\u0027 It says, \u0027Because the model is so articulate and confident, we develop an “automation bias”: we just assume it’s right. (“ChatGPT told me so”)... This is *dangerous*.\u0027"
    },
    {
        "question":  "In the proposed software development framework, what happens in \u0027The results\u0027 step after the agent implements the plan?",
        "options":  [
                        "The project is finished and deployed immediately",
                        "The human developer must rewrite all the code from scratch",
                        "The human checks the results, and if something is not as expected, they \u0027re-iterate the process\u0027",
                        "The LLM automatically fires the human developer"
                    ],
        "correct_answer":  "The human checks the results, and if something is not as expected, they \u0027re-iterate the process\u0027",
        "motivation":  "The final step of the framework, \u0027The results\u0027, is described as checking the agent\u0027s implementation and tests. The text states: \u0027In case something doesn’t go the way you expected… Well, this is where you re-iterate the process.\u0027"
    },
    {
        "question":  "What is the statistical \u0027Mean\u0027?",
        "options":  [
                        "The middle value when data is sorted; robust to outliers",
                        "The \u0027average\u0027 of a dataset; prone to being skewed by outliers",
                        "How spread out the data is",
                        "The square root of the variance"
                    ],
        "correct_answer":  "The \u0027average\u0027 of a dataset; prone to being skewed by outliers",
        "motivation":  "The text defines \u0027Mean\u0027 in the \u0027Probability and Statistics\u0027 section as \u0027The “average”. Prone to being skewed by outliers.\u0027"
    },
    {
        "question":  "What is the \u0027Variance\u0027 in statistics?",
        "options":  [
                        "The \u0027average\u0027 of a dataset",
                        "The middle value when data is sorted",
                        "A measure of how spread out the data is",
                        "The \u0027ground truth\u0027 error"
                    ],
        "correct_answer":  "A measure of how spread out the data is",
        "motivation":  "The text lists \u0027Variance\u0027 under its statistical definitions, explaining it as \u0027How spread out the data is.\u0027 It is listed alongside Standard Deviation, which is the square root of variance."
    }
]
